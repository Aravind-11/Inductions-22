{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwHaRJpruXsz"
      },
      "source": [
        "!pip install -q snscrape==0.3.4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nD7B3adiubGG"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from datetime import date"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNhZNSNavwGQ"
      },
      "source": [
        "today = date.today()\n",
        "end_date = today"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUFINzS2yNem"
      },
      "source": [
        "search_term = 'Elon Musk'\n",
        "from_date = '2022-09-01'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dx9KjHTi4Fxj"
      },
      "source": [
        "# Total Number of Tweets for Search Terms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0fFOKpLwBGF",
        "outputId": "b2a02763-e7be-4f3a-945a-316e9337109d"
      },
      "source": [
        "os.system(f\"snscrape --since {from_date} twitter-search '{search_term} until:{end_date}' > result-tweets.txt\")\n",
        "if os.stat(\"result-tweets.txt\").st_size == 0:\n",
        "  counter = 0\n",
        "else:\n",
        "  df = pd.read_csv('result-tweets.txt', names=['link'])\n",
        "  counter = df.size\n",
        "\n",
        "print('Number Of Tweets : '+ str(counter))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number Of Tweets : 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFN4VxqK4KDU"
      },
      "source": [
        "# Extracting Exact Trending Tweeets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGUawFA0xTG3"
      },
      "source": [
        "max_results = 1000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWseHfDmwYCw",
        "outputId": "6fd2cab6-d879-4517-c6e0-53a185fe8f0f"
      },
      "source": [
        "extracted_tweets = \"snscrape --format '{content!r}'\"+ f\" --max-results {max_results} --since {from_date} twitter-search '{search_term} until:{end_date}' > extracted-tweets.txt\"\n",
        "os.system(extracted_tweets)\n",
        "if os.stat(\"extracted-tweets.txt\").st_size == 0:\n",
        "  print('No Tweets found')\n",
        "else:\n",
        "  df = pd.read_csv('extracted-tweets.txt', names=['content'])\n",
        "  for row in df['content'].iteritems():\n",
        "    print(row)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "((\"'üö®Tech Talk of the Week alert!\\\\n\\\\nLearn about TensorFlow Hub by joining the session hosted by Machine Learning GDE\", ' and Data Scientist'), \" Bhavesh Bhatt.\\\\n\\\\nThis event will start at 8 PM Oman time\\\\n\\\\nJoin by clicking the link below!\\\\nhttps://t.co/CO6S9i6NfL\\\\n\\\\n#CourageToCreate #IWD #WTM https://t.co/4zKKFtfuhC https://t.co/qs8N3SwYW3'\")\n",
            "((\"'This Sunday\", ' April 18\\\\n\\\\nüíæTech Talk Time!\\\\n\\\\nFrom 8 to 9PM GST'), ' learn about TFHub\\\\n\\\\nTensorFlow Hub is an open repository &amp; library for reusable machine learning. Come join Machine Learning GDE &amp; Data Scientist')\n",
            "((\"'Data Science Career | How to Transition to Data Science with Data Scientist Bhavesh Bhatt |GreyAtom https://t.co/ocZCP6LCeM'\", nan), nan)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWWTBnwS4Ofr"
      },
      "source": [
        "# Extracting Tweets from Specific Users"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFBwPec2zSr6",
        "outputId": "106991a9-2325-48c7-f650-dc948c510b8e"
      },
      "source": [
        "user_name = \"elonmusk\"\n",
        "user_tweets = \"snscrape --format '{content!r}'\"+ f\" --max-results {max_results} --since {from_date} twitter-user '{user_name} until:{end_date}' > user-tweets.txt\"\n",
        "os.system(user_tweets)\n",
        "if os.stat(\"Elonmusktweets.txt\").st_size == 0:\n",
        "  print('No Tweets found')\n",
        "else:\n",
        "  df = pd.read_csv('Elonmusktweets.txt', names=['content'])\n",
        "  for row in df['content'].iteritems():\n",
        "    print(row)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(0, \"@iaveshh @huggingface I'm still exploring the M1 so don't want to create a video for the sake of creating it. I'll create a video once I have enough data points.\")\n",
            "(1, \"@iaveshh Hey Avesh, I'm still exploring the device so won't be able to comment right away but I'll definitely make a video on this soon :)\")\n",
            "(2, \"'55% battery consumed with ~8 hours of screen on time with close to 4-5 hours of 4k video consumption.\\\\n\\\\nIs this a dream or are we living in the future? \\\\n\\\\n#mindblown #m1 #mac #M1Mac #MacBookAir #macbook https://t.co/uKnYUTdqPI'\")\n",
            "(3, \"'The M1 MacBook Air is a Beast. I just rendered a 2 minute video in less than 35 seconds.\\\\n\\\\n#mindblown #m1 #mac #M1Mac #videoediting #MacBookAir #macbook https://t.co/5tFBdI1B9G'\")\n",
            "(4, \"'@RsreeTech @PrabhhavSharma Thanks for sharing this :)'\")\n",
            "(5, \"'Just received some super amazing package from #Google for the talk that I gave during #GoogleIO this yearüôÇ\\\\n\\\\nThank you @GoogleDevsIN &amp; @GoogleDevExpert üôÇ\\\\n\\\\n#GDE https://t.co/WsI2ChK56B'\")\n",
            "(6, \"@PrabhhavSharma I'm without a laptop currently üòõ As soon as I buy a laptop, I'll create a video on it soon!\")\n",
            "(7, \"'Lenovo Thinkpad Yoga 14 was a friend that helped me for 5.5 long years. This very laptop helped me create 300+ videos &amp; propelled my career growth. You will definitely be missed. Thank you #Lenovo for creating such a solid product. https://t.co/6lQ993hHnv'\")\n",
            "(8, '\\'Chandler Bing\\\\\\'s job was \"Data Reconfiguration &amp; Statistical Factoring\" so officially he became a Data Scientist long backüòÇ\\\\n\\\\n#AI #ArtificialIntelligence #ML #MachineLearning #DeepLearning #DataScience #FriendsReunion\\'')\n",
            "(9, \"It's getting to that stage that by the time you learn a technique in ML &amp; then a year later it's obsolete. LSTM is now made obsolete all thanks to Transformers. Its tough to keep up with the advances.\\\\n\\\\n#AI #ArtificialIntelligence #ML #MachineLearning #DeepLearning #DataScience\")\n",
            "(10, \"It‚Äôs finally happening. I'm thrilled to be featured at #GoogleIO this year in a lighting talk on Landmark Detection using TensorFlow Hub.\\\\n\\\\nIt's free &amp; virtual. Click here to register: \\\\nhttps://t.co/AJlXOijvow\\\\n\\\\nSee you there :)\\\\n\\\\n@GoogleDevExpert @GDGIndia @googledevs @GoogleDevsIN\")\n",
            "(11, \"@ankur_rangi The API calls are being blocked! I'm trying to figure out a way!\")\n",
            "(12, \"'@philipvollet Thank you üôè'\")\n",
            "(13, \"'30K Subscribers! Thank Youüôè\\\\n\\\\n#YouTube #youtubechannel #DataScience #MachineLearning #AI #ArtificialIntelligence #DeepLearning https://t.co/YR0o0ss0lr'\")\n",
            "(14, \"'@bengaluruu @rnaik21 Try now!'\")\n",
            "(15, \"'@malind Sure! I will add that feature'\")\n",
            "(16, \"@sss26888 I haven't tried it! I will try to add this feature soon.\")\n",
            "(17, \"'@RameshYesh Look at the recent commits :)'\")\n",
            "(18, \"'New Video : Derivative of the Tanh Activation function in Deep Learning\\\\n\\\\nVideo Link : https://t.co/DyV6f2cG9z\\\\n\\\\n#DeepLearning #DataScience #Python #AI #MachineLearning #ArtificialIntelligence'\")\n",
            "(19, \"'NVIDIA GTC 21 is live now\\\\n\\\\n#GTC21 #GTCwithMe #Nvidia https://t.co/R78aLU9vuK'\")\n",
            "(20, \"New Video : @streamlit application summarize text on blogs, websites using @huggingface's summarization pipeline.\\\\n\\\\nVideo Link : https://t.co/qbTmdABC0z\\\\n\\\\n#DeepLearning #DataScience #Python #AI #MachineLearning #ArtificialIntelligence https://t.co/gPafBU5Iln\")\n",
            "(21, \"'haha üòÇ https://t.co/aFDKhFC41I'\")\n",
            "(22, \"New Video : @streamlit application to answer questions regarding your text using @huggingface's q&amp;a pipeline.\\\\n\\\\nVideo Link : https://t.co/AB9couvIDs\\\\n\\\\n#DeepLearning #DataScience #Python #AI #MachineLearning #ArtificialIntelligence https://t.co/kWPrfqpAmY\")\n",
            "(23, \"'New Video : @streamlit application to detect tables in document images such as invoices\")\n",
            "(24, \"'Palindrome Subscribers :)\\\\n\\\\n#YouTube #youtubechannel #DataScience #MachineLearning #AI #ArtificialIntelligence #DeepLearning https://t.co/Qqfs9W8dSm'\")\n",
            "(25, \"'NEW Video - Automatic Regression using TuringBot software.\\\\n\\\\nVideo Link : https://t.co/IVnWnGHJxw\\\\n\\\\n#DataScience #MachineLearning #ArtificialIntelligence #AI #YouTube #DeepLearning https://t.co/UgkY2ZBXlc'\")\n",
            "(26, \"'New Video : I created a #Bitcoin tracking application with 20 lines of Python code using @streamlit \\\\n\\\\nVideo Link : https://t.co/IwZkdFmgFW\\\\n\\\\n#DeepLearning #DataScience #Python #AI #MachineLearning #ArtificialIntelligence https://t.co/oslKpc9FRq'\")\n",
            "(27, \"'NEW Video - Answer Questions related to a table using @huggingface Transformers Pipeline.\\\\n\\\\nVideo Link : https://t.co/5rynXzNruu\\\\n\\\\n#DataScience #MachineLearning #ArtificialIntelligence #AI #YouTube #DeepLearning #NLP https://t.co/XWEvwbf1J1'\")\n",
            "(28, \"'NEW Video - Sentiment Analysis using @huggingface Transformers Pipeline\\\\n\\\\nVideo Link : https://t.co/A8epjff4VA\\\\n\\\\n#DataScience #MachineLearning #ArtificialIntelligence #AI #YouTube #DeepLearning #NLP https://t.co/3sVMlWnEPA'\")\n",
            "(29, \"'NEW Video - NVIDIA Deep Learning Institute (DLI) Course Giveaway Results.\\\\n\\\\nVideo Link : https://t.co/mtI5D4w5i0 \\\\n\\\\n#DataScience #MachineLearning #ArtificialIntelligence #AI #YouTube #DeepLearning https://t.co/Hko3Zi5krh'\")\n",
            "(30, \"'NEW Video - Free @nvidia GTC21 Conference + #nvidia Deep Learning Institute Course Giveaway.\\\\n\\\\nVideo Link : https://t.co/XtrJLp2505\\\\n\\\\n@NVIDIAAI @NVIDIAGTC #nvidia #GTC21 #AI #GTCwithme #Conference #DataScience https://t.co/BGFRrmSOUm'\")\n",
            "(31, \"I am planning to do a small giveaway as I'm about to reach 29000 subscribers.\\\\n\\\\nThe giveaway will help you in your career going forward. \\\\n\\\\nStay tuned for my next video :)\\\\n\\\\n#DataScience #MachineLearning #ArtificialIntelligence #AI #YouTube #DeepLearning https://t.co/Hm8mwg5fiT\")\n",
            "(32, \"'NEW Video - Optical Character Recognition (OCR) in Python using keras-ocr\\\\n\\\\nVideo Link : https://t.co/ZlH5gkQvEL\\\\n\\\\n#MachineLearning #DeepLearning #DataScience #Python #AI https://t.co/VRmVrVVpWZ'\")\n",
            "(33, \"'NEW Video - Remove Background Noise using #NVIDIA RTX Voice.\\\\n\\\\nVideo Link : https://t.co/vHVKWFUY2Y\\\\n\\\\n#MachineLearning #DeepLearning #DataScience #Python #AI https://t.co/JOjIMEUeHY'\")\n",
            "(34, \"'When you get @TensorFlow swags üòé\\\\n\\\\nThank you @GoogleDevsIN &amp; @GoogleDevExpert üòÄ \\\\n\\\\n#GDE https://t.co/2iT0IJ9wEE'\")\n",
            "(35, \"'Apply Quickly :) #AcceleratedWithGoogle\\\\n\\\\n@GoogleDevsIN @GoogleDevExpert https://t.co/o8l1Ale2DH'\")\n",
            "(36, \"'NEW Video - TensorGram : Telegram bot to receive Deep Learning model training updates on your mobile.\\\\n\\\\nVideo Link : https://t.co/TwI835oZMw\\\\n\\\\n#MachineLearning #DeepLearning #DataScience #Python #AI https://t.co/42Q49j5pTf'\")\n",
            "(37, \"'https://t.co/fO6aehIwnZ'\")\n",
            "(38, \"New Video : Accelerate NumPy operations using @TensorFlow's tf.experimental.numpy module &amp; @nvidia's GPUs\\\\n\\\\nVideo Link : https://t.co/Snhi5iw8i1 \\\\n\\\\n@GoogleDevExpert @GoogleDevExpert \\\\n\\\\n#ArtificialIntelligence #MachineLearning #Python #AI https://t.co/iQ6onNGzhH\")\n",
            "(39, \"New Video : Is this the BEST BOOK on Google's BERT?\\\\n\\\\nVideo Link : https://t.co/5GhN3XEEe7\\\\n\\\\n#MachineLearning #DeepLearning #DataScience #Python #AI https://t.co/Q9tyz99lJ7\")\n",
            "(40, \"'NVIDIA is definitely increasing AI adoption. NVIDIA just announced : NVIDIA AI Enterprise\")\n",
            "(41, \"'Video Tutorial : A simple spelling &amp; grammar checker web application using Python \\\\n\\\\nVideo Link : https://t.co/gU7AP6x3iu\\\\n\\\\n#ArtificialIntelligence #MachineLearning #AI #DataScience #NLP https://t.co/TW5lSYyy0g'\")\n",
            "(42, \"NEW Video - Easy to use extractive text summarization using bert-extractive-summarizer which uses @huggingface's neuralcoref library.\\\\n\\\\nVideo Link : https://t.co/ocysr4zgFh\\\\n\\\\n#MachineLearning #DeepLearning #DataScience #Python #AI https://t.co/G6RS4tdloU\")\n",
            "(43, \"'I created a mini version of Grammarly using #Python &amp; @streamlit.\\\\n\\\\nVideo Tutorial coming soon :)\\\\n\\\\n#ArtificialIntelligence #MachineLearning #AI #DataScience #NLP https://t.co/HVCqTgyF0S'\")\n",
            "(44, \"'NEW Video - NVIDIA Jarvis\")\n",
            "(45, '\\'Charles Cooley : \"I am not what I think I am')\n",
            "(46, \"'NEW Video - Notify : Jupyter Extension For Browser Notifications of Cell Completion in Jupyter Notebook\\\\n\\\\nVideo Link : https://t.co/7pJKQbUIJs\\\\n\\\\n#MachineLearning #DeepLearning #DataScience #Python #AI https://t.co/pa1q4He5Mc'\")\n",
            "(47, \"NEW Video : @facebookai's mBART-50  using @huggingface's transformer for Multilingual Language Translation.\\\\n\\\\nVideo Link : https://t.co/wADHZYLnqR\\\\n\\\\n#NLP #MachineLearning #DeepLearning #DataScience #Python #AI https://t.co/ok6I7aiGK2\")\n",
            "(48, \"'I created a @streamlit application that translates from Hindi to English. A Big Shoutout to @huggingface &amp; @facebookai for open sourcing mBART-50 which helps in Translating text to\")\n",
            "(49, \"'Launch VSCode (codeserver) on Google Colab using ColabCode!\\\\n\\\\nThank you @abhi1thakur for creating this :)\\\\n\\\\nVideo Link : https://t.co/xTQFeQBdNS\\\\n\\\\n#deployment #google #vscode #machinelearning #DataScience https://t.co/WUF9cmRXmy'\")\n",
            "(50, \"'12-02-2021 - Palindrome Day!'\")\n",
            "(51, \"@facebookai's Wav2Vec 2.0 using @huggingface's transformer 4.3.0 for Automatic Speech Recognition.\\\\n\\\\nNow, you can transcribe your audio files directly on the hub using Wav2Vec2.\\\\n\\\\nVideo Link : https://t.co/E0omr18jIx\\\\n\\\\n#NLP #MachineLearning #DeepLearning #DataScience #Python #AI https://t.co/mXww5PQNHs\")\n",
            "(52, \"'@MdArif34 Thank you so much!'\")\n",
            "(53, \"'1000 Followers on GitHub üôèüôè\\\\n\\\\n#github #MachineLearning #AI #YouTube https://t.co/puNy3UuSLk'\")\n",
            "(54, \"FREE Data Science Online Course for Absolute Beginner's\\\\n\\\\nVideo Link : https://t.co/FyWRS4iANO\\\\n\\\\n#DeepLearning #DataScience #Python #AI #MachineLearning #ArtificialIntelligence https://t.co/yeOeBxsaZ6\")\n",
            "(55, \"'A Big Big Thank You to @GoogleDevsIN &amp; @GoogleDevExpert for helping me have a higher quality online presence by providing me..üôÇ\\\\n- Blue Yeti USB Mic + Pop Filter\\\\n- Lenovo HD Web Camera\\\\n- Ring Light with Stand \\\\n\\\\n#GDE #DataScience #MachineLearning #AI #YouTube #youtubechannel https://t.co/azhYLRI2Hh'\")\n",
            "(56, \"'From 36 All out to winning the test series. \\\\nHistory has been made ‚úåÔ∏è‚úåÔ∏è‚úåÔ∏è‚úåÔ∏è‚úåÔ∏è\\\\n#AUSvIND #GabbaTest #Gabba #AUSvINDtest'\")\n",
            "(57, \"'History will definitely be made today üáÆüá≥ü§û\\\\n#AUSvIND #GabbaTest #Gabba #AUSvINDtest'\")\n",
            "(58, \"'Best Data Science Certifications from Google to consider in 2021.\\\\n\\\\nVideo Link : https://t.co/hCiKd8K2Qb\\\\n\\\\n#DeepLearning #DataScience #Python #AI #MachineLearning https://t.co/EvGwLkL0ys'\")\n",
            "(59, \"'Will AutoML replace a Data Scientist?\\\\n\\\\nVideo Link : https://t.co/G3EBbGU6Jn\\\\n\\\\n#DeepLearning #DataScience #Python #AI #MachineLearning https://t.co/jwjItpuj7P'\")\n",
            "(60, \"'Faster Pandas Operation using PyPolars\\\\n\\\\nVideo Link : https://t.co/T2ohYevMIC\\\\n\\\\n#DeepLearning #DataScience #Python #AI #MachineLearning #ArtificialIntelligence https://t.co/ie4rTD1uB5'\")\n",
            "(61, \"'Language Identification using Google Compact Language Detector v3 (CLD3)\")\n",
            "(62, \"'No Facebook\")\n",
            "(63, \"'TextHero : Simplest way to Clean &amp; Analyze Text Data in Pandas\\\\n\\\\nVideo Link : https://t.co/757ENLb0Sr\\\\n\\\\n#DeepLearning #DataScience #Python #AI #MachineLearning #ArtificialIntelligence https://t.co/MVDUXY8Bon'\")\n",
            "(64, \"Cleaning Text Data using Python's Clean-Text Library\\\\n\\\\nVideo Link : https://t.co/FBnoWvWLVP\\\\n\\\\n#DeepLearning #DataScience #Python #AI #MachineLearning #artificialitellegence https://t.co/0ea3t6OqA9\")\n",
            "(65, \"'Simplest Example to Gamma &amp; C Hyper parameters of SVM.\\\\n\\\\nVideo Link : https://t.co/QwAx7ATiZp\\\\n\\\\n#DeepLearning #DataScience #Python #AI #MachineLearning #ArtificialIntelligence https://t.co/6cZwaBaQsS'\")\n",
            "(66, \"'Auto_TS : Automatically build multiple Time Series models using a Single Line of Code.\\\\n\\\\nVideo Link : https://t.co/k6DxUGy7ks\\\\n\\\\n#DeepLearning #DataScience #Python #AI #MachineLearning #ArtificialIntelligence https://t.co/37ZCHhncqo'\")\n",
            "(67, \"'The insights that a single alluvial diagram can give is amazing üòÄ https://t.co/FsNFhj0mv9'\")\n",
            "(68, \"'Topic Modeling with BERT using Top2Vec.\\\\n\\\\nVideo Link : https://t.co/U8Rrj9wpEq\\\\n\\\\n#DeepLearning #DataScience #Python #AI #MachineLearning #ArtificialIntelligence #NLP https://t.co/YUSxDAH95p'\")\n",
            "(69, \"'I had an amazing experience being interviewed by @tejakkuntla on the Exploiting Podcast. \\\\n\\\\nFind the episode at: https://t.co/CYMcXBpjBp\\\\n\\\\n#MachineLearning #DeepLearning #DataScience #Python #AI https://t.co/a3a87woKyq'\")\n",
            "(70, \"Today's draw feels like a win! üôè #INDvAUS\")\n",
            "(71, \"'Lazy Predict for ML Models (AutoML)\\\\n\\\\nVideo Link : https://t.co/6vAqzhVkgZ\\\\n\\\\n#DeepLearning #DataScience #Python #AI #MachineLearning #ArtificialIntelligence https://t.co/A6GLAHvcz6'\")\n",
            "(72, \"'Regression using Multivariate Adaptive Regression Splines (MARS)\\\\n\\\\nVideo Link : https://t.co/Mm0UJk0fQW\\\\n\\\\n#DeepLearning #DataScience #Python #AI #MachineLearning #ArtificialIntelligence https://t.co/8SaJk5b95s'\")\n",
            "(73, \"'Topic Modeling with BERT using BERTopic.\\\\n\\\\nVideo Link : https://t.co/0lk1zSiqBF\\\\n\\\\n#DeepLearning #DataScience #Python #AI #MachineLearning #ArtificialIntelligence #NLP https://t.co/R5tRcbVz5W'\")\n",
            "(74, \"'OpenAI has created the DALL-E model\")\n",
            "(75, \"'Low Light Image Enhancement using Python &amp; Deep Learning\\\\n\\\\nVideo Link : https://t.co/AImtQsbWGe\\\\n\\\\n#DeepLearning #DataScience #Python #AI #MachineLearning #ArtificialIntelligence https://t.co/dUhtN47A3H'\")\n",
            "(76, \"'Visualize Python Code Execution.\\\\n\\\\nVideo Link : https://t.co/h7ORg619eZ\\\\n\\\\n#programmer #Python #Python3 #pythonlearning #pythoncode #SoftwareEngineer https://t.co/Kgk3pDIs75'\")\n",
            "(77, \"Happy New Year everyone ü•≥ I hope 2021 turns out to be the best year of your life &amp; your family too. 2020 was a hard year because of COVID-19. Let's hope 2021 brings an end to this COVID-19 menace. Please stay safe &amp; look after one another. Bhavesh ‚ú®‚ú®\")\n",
            "(78, \"In 2021, I want to get my hands on the @YouTube's Silver Play Button ‚ú®‚ú®ü§ûü§û\\\\n\\\\n#YouTube #youtubechannel #DataScience #MachineLearning #AI #ArtificialIntelligence #DeepLearning\")\n",
            "(79, \"'@Arth_Soni242001 This blog should help - https://t.co/GFgbKLD70y'\")\n",
            "(80, \"'Wishing all of you &amp; your families a Merry Christmas &amp; Happy Holidays üéÖ üéÑ\\\\n\\\\nA lot happened in 2020\\\\n- I created 115 videos &amp; gained close to 19k subscribers üôè\\\\n- I was awarded the 40 Under 40 Data Scientist award.\\\\n\\\\nThank you all for the support in 2020 &amp; looking forward to 2021üòä'\")\n",
            "(81, \"@Arth_Soni242001 @GoogleDevsIN @GoogleDevExpert @sidagarwal04 I'm a GDE in Machine Learning Arth so we get rewarded for the community work that we do by Google :)\")\n",
            "(82, \"'Received some more awesome year end gifts from Google :) Thank you @GoogleDevsIN \")\n",
            "(83, \"'I have recently developed a fondness towards Palindrome Numbers\")\n",
            "(84, \"'My small tutorial on BERT based MuRIL (Multilingual Representations for Indian Languages) by @GoogleIndia\\\\n\\\\nVideo Link : https://t.co/neEL9AT8ob\\\\n\\\\n@GoogleDevsIN @GoogleDevExpert\\\\n\\\\n#DeepLearning #DataScience #Python #AI #MachineLearning #ArtificialIntelligence #NLP https://t.co/NpoXNFpjiy'\")\n",
            "(85, \"'#Bitcoin smashed through $20\")\n",
            "(86, \"'Simplest Example to explain the advantages of BERT over Word2Vec models.\\\\n\\\\nVideo Link : https://t.co/OvG1ckHZKa\\\\n\\\\n#DeepLearning #DataScience #Python #AI #MachineLearning #ArtificialIntelligence https://t.co/xC8tqr5vwJ'\")\n",
            "(87, \"'Received some amazing year end gifts :) Thank you @GoogleDevsIN \")\n",
            "(88, \"'Scrape HTML tables easily with a Button Click using Python!\\\\n\\\\nVideo Link : https://t.co/1ttzsSw7ua\\\\n\\\\n#DeepLearning #DataScience #Python #AI #MachineLearning #ArtificialIntelligence https://t.co/k6GOTri66c'\")\n",
            "(89, \"I sometimes can't imagine my life without Google Colab! üôÇ https://t.co/z2AnzwjW2G\")\n",
            "(90, \"'Turn Your Photos into Pencil Drawing/Sketch Easily using Python.\\\\n\\\\nVideo Link : https://t.co/svkDKfxB4n\\\\n\\\\n#DeepLearning #DataScience #Python #AI #MachineLearning #ArtificialIntelligence https://t.co/uEWHMtKs0x'\")\n",
            "(91, \"'Fuzzy String Matching with BERT\")\n",
            "(92, \"'For all of you who wanted to support me directly for my YouTube videos.\\\\n\\\\nI have activated the YouTube membership option. Do check out this video for more information üôÇ\\\\n\\\\nLink : https://t.co/7PDERftqlK\\\\n\\\\n#DataScience #MachineLeraning #YouTube #ArtificialIntelligence #teaching https://t.co/wiWteSMIDW'\")\n",
            "(93, \"'Learn how to create Ridgeline plots which shows the distribution of a numeric value for several groups. \\\\n\\\\nVideo Link : https://t.co/H9vssCWslK\\\\n\\\\n#DeepLearning #DataScience #Python #AI #MachineLearning https://t.co/UaRWEWVdK8'\")\n",
            "(94, \"'Elegant Neural Network User Interface to build drag-and-drop neural networks\")\n",
            "(95, \"'@AiCodist Docly is surely gonna go places :) Thanks for creating it!'\")\n",
            "(96, \"Auto-Generate Python Comments/Documentation using with @AiCodist's Docly\\\\n\\\\nVideo Link : https://t.co/FO8E8jQ80U\\\\n\\\\n#Docly #NLP #MachineLearning #DeepLearning #DataScience #Python #AI https://t.co/2Fnj2xmcgG\")\n",
            "(97, \"'The small details of his videos üôè https://t.co/OTav4A2Wny'\")\n",
            "(98, \"I started a channel with the aim of teaching Python in 2020 &amp; I just realized that it has crossed 2000 subscriber mark üôÇ\\\\n\\\\nDo check the channel if you haven't already : https://t.co/ZeTDCXoMHI\\\\n\\\\n#Python #DataScience #MachineLearning #AI  #DeepLearning #youtubechannel https://t.co/XwQwy2FOlP\")\n",
            "(99, \"November, 2017 was when I uploaded my first video on YouTube. Very little did I know that in exactly 3 years, I'll gain 25K Subscribers! A Big Thank You to all of you :)\\\\n\\\\n#DataScience #MachineLearning #AI #Python #youtubechannel #YouTube #YouTuber #YouTubers https://t.co/kIIUHArXkJ\")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Split the Dataset**\n"
      ],
      "metadata": {
        "id": "Af6H4Sdglowt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from tqdm import tqdm\n",
        "!mkdir output\n",
        "\n",
        "try:\n",
        "    full_dataset = open(\"Elonmusktweets.csv\", \"r\")\n",
        "    pos_dataset = open(\"tw-data.pos\", \"w\")\n",
        "    neg_dataset = open(\"tw-data.neg\", \"w\")\n",
        "except IOError:\n",
        "    print(\"Failed to open file\")\n",
        "    quit()\n",
        "\n",
        "csv_lines = full_dataset.readlines()\n",
        "i = 0.0\n",
        "\n",
        "for line in tqdm(csv_lines):\n",
        "    i += 1.0\n",
        "    line = line.split(\",\", 3)\n",
        "    tweet = line[2].strip()\n",
        "    new_tweet = ''\n",
        "\n",
        "    for word in tweet.split():\n",
        "        # String preprocessing\n",
        "        if re.match('^.*@.*', word):\n",
        "            word = '<NAME/>'\n",
        "        if re.match('^.*http://.*', word):\n",
        "            word = '<LINK/>'\n",
        "        word = word.replace('#', '<HASHTAG/> ')\n",
        "        word = word.replace('&quot;', ' \\\" ')\n",
        "        word = word.replace('&amp;', ' & ')\n",
        "        word = word.replace('&gt;', ' > ')\n",
        "        word = word.replace('&lt;', ' < ')\n",
        "        new_tweet = ' '.join([new_tweet, word])\n",
        "\n",
        "    tweet = new_tweet.strip() + '\\n'\n",
        "\n",
        "    if line[1].strip() == '1':\n",
        "        pos_dataset.write(tweet)\n",
        "    else:\n",
        "        neg_dataset.write(tweet)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eG68oxXC20w0",
        "outputId": "ac7b048c-8fc5-4d0f-ca1a-bb022c492d3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‚Äòoutput‚Äô: File exists\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 959/959 [00:00<00:00, 32793.38it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Helper Functions**"
      ],
      "metadata": {
        "id": "seiYkoXcmWFG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import random\n",
        "import csv\n",
        "\n",
        "POS_DATASET_PATH = '/content/tw-data.pos'\n",
        "NEG_DATASET_PATH = '/content/tw-data.neg'\n",
        "VOC_PATH = '/content/vocab.csv'\n",
        "VOC_INV_PATH = '/content/vocab_inv.csv'\n",
        "\n",
        "\n",
        "def clean_str(string):\n",
        "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
        "    string = re.sub(r'(.)\\1+', r'\\1\\1', string)\n",
        "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
        "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
        "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
        "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
        "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
        "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
        "    string = re.sub(r\",\", \" , \", string)\n",
        "    string = re.sub(r\"!\", \" ! \", string)\n",
        "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
        "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
        "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
        "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
        "    return string.strip().lower()\n",
        "\n",
        "\n",
        "def sample_list(list, fraction):\n",
        "    return random.sample(list, int(len(list) * fraction))\n",
        "\n",
        "\n",
        "def load_data_and_labels(dataset_fraction = 1.0):\n",
        "    print(\"\\tdata_helpers: loading positive examples...\")\n",
        "    positive_examples = list(open(POS_DATASET_PATH).readlines())\n",
        "    positive_examples = [s.strip() for s in positive_examples]\n",
        "    print(\"\\tdata_helpers: [OK]\")\n",
        "    print(\"\\tdata_helpers: loading negative examples...\")\n",
        "    negative_examples = list(open(NEG_DATASET_PATH).readlines())\n",
        "    negative_examples = [s.strip() for s in negative_examples]\n",
        "    print(\"\\tdata_helpers: [OK]\")\n",
        "\n",
        "    positive_examples = sample_list(positive_examples, dataset_fraction)\n",
        "    negative_examples = sample_list(negative_examples, dataset_fraction)\n",
        "\n",
        "    # Split by words\n",
        "    x_text = positive_examples + negative_examples\n",
        "    print(\"\\tdata_helpers: cleaning strings...\")\n",
        "    x_text = [clean_str(sent) for sent in x_text]\n",
        "    x_text = [s.split() for s in x_text]\n",
        "    print(\"\\tdata_helpers: [OK]\")\n",
        "\n",
        "    # Generate labels\n",
        "    print(\"\\tdata_helpers: generating labels...\")\n",
        "    positive_labels = [[0, 1] for _ in positive_examples]\n",
        "    negative_labels = [[1, 0] for _ in negative_examples]\n",
        "    print(\"\\tdata_helpers: [OK]\")\n",
        "    print(\"\\tdata_helpers: concatenating labels...\")\n",
        "    y = np.concatenate([positive_labels, negative_labels], 0)\n",
        "    print(\"\\tdata_helpers: [OK]\")\n",
        "    return [x_text, y]\n",
        "\n",
        "\n",
        "def pad_sentences(sentences, padding_word=\"<PAD/>\"):\n",
        "    sequence_length = max(len(x) for x in sentences)\n",
        "    padded_sentences = []\n",
        "    for i in range(len(sentences)):\n",
        "        sentence = sentences[i]\n",
        "        num_padding = sequence_length - len(sentence)\n",
        "        new_sentence = sentence + [padding_word] * num_padding\n",
        "        padded_sentences.append(new_sentence)\n",
        "    return padded_sentences\n",
        "\n",
        "\n",
        "def pad_sentences_to(sentences, pad_to, padding_word=\"<PAD/>\"):\n",
        "    sequence_length = pad_to\n",
        "    padded_sentences = []\n",
        "    for i in range(len(sentences)):\n",
        "        sentence = sentences[i]\n",
        "        num_padding = sequence_length - len(sentence)\n",
        "        new_sentence = sentence + [padding_word] * num_padding\n",
        "        padded_sentences.append(new_sentence)\n",
        "    return padded_sentences\n",
        "\n",
        "\n",
        "def build_vocab():\n",
        "\n",
        "    voc = csv.reader(open(VOC_PATH))\n",
        "    voc_inv = csv.reader(open(VOC_INV_PATH))\n",
        "    # Mapping from index to word\n",
        "    vocabulary_inv = [x for x in voc_inv]\n",
        "    # Mapping from word to index\n",
        "    vocabulary = {x: i for x, i in voc}\n",
        "    return [vocabulary, vocabulary_inv]\n",
        "\n",
        "\n",
        "def build_input_data(sentences, labels, vocabulary):\n",
        "\n",
        "    x = np.array([[vocabulary[word] for word in sentence] for sentence in sentences])\n",
        "    y = np.array(labels)\n",
        "    return [x, y]\n",
        "\n",
        "\n",
        "def string_to_int(sentence, vocabulary, max_len):\n",
        "    # Reads dataset in order to create the vocabulary\n",
        "    base = [sentence]\n",
        "    base = [s.strip() for s in base]\n",
        "    x_text = base\n",
        "    x_text = [clean_str(sent) for sent in x_text]\n",
        "    x_text = [s.split(\" \") for s in x_text]\n",
        "    padded_x_text = pad_sentences_to(x_text, max_len)\n",
        "    try:\n",
        "        x = np.array([[vocabulary[word] for word in sentence] for sentence in padded_x_text])\n",
        "        return x\n",
        "    except KeyError as e:\n",
        "        print(\"The following word is unknown to the network: %s\" % str(e))\n",
        "        quit()\n",
        "\n",
        "\n",
        "def load_data(dataset_fraction = 1.0):\n",
        "    # Load and preprocess data\n",
        "    sentences, labels = load_data_and_labels(dataset_fraction)\n",
        "    print(\"\\tdata_helpers: padding strings...\")\n",
        "    sentences_padded = pad_sentences(sentences)\n",
        "    print(\"\\tdata_helpers: [OK]\")\n",
        "    print(\"\\tdata_helpers: building vocabulary...\")\n",
        "    vocabulary, vocabulary_inv = build_vocab()\n",
        "    print(\"\\tdata_helpers: [OK]\")\n",
        "    print(\"\\tdata_helpers: building processed datasets...\")\n",
        "    x, y = build_input_data(sentences_padded, labels, vocabulary)\n",
        "    print(\"\\tdata_helpers: [OK]\")\n",
        "    return [x, y, vocabulary, vocabulary_inv]\n",
        "\n",
        "\n",
        "def batch_iter(data, batch_size, num_epochs):\n",
        "    data = np.array(data)\n",
        "    data_size = len(data)\n",
        "    num_batches_per_epoch = int(len(data)/batch_size) + 1\n",
        "    for epoch in range(num_epochs):\n",
        "        # Shuffle the data at each epoch\n",
        "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
        "        shuffled_data = data[shuffle_indices]\n",
        "        for batch_num in range(num_batches_per_epoch):\n",
        "            start_index = batch_num * batch_size\n",
        "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
        "            yield shuffled_data[start_index:end_index]"
      ],
      "metadata": {
        "id": "GHs86_fJ6re3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Extract Vocabulary**"
      ],
      "metadata": {
        "id": "xat-BtnwmKaA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracts vocabulary from the tweet dataset and can Potentially be used for future word cloud building"
      ],
      "metadata": {
        "id": "HzQoXWQr4v5r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import itertools\n",
        "from collections import Counter\n",
        "import csv\n",
        "\n",
        "\n",
        "def build_vocab(sentences):\n",
        "    # Build vocabulary\n",
        "    word_counts = Counter(itertools.chain(*sentences))\n",
        "    # Mapping from index to word\n",
        "    vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
        "    # Mapping from word to index\n",
        "    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n",
        "    return [vocabulary, vocabulary_inv]\n",
        "\n",
        "\n",
        "# Load and preprocess data\n",
        "print('vocab_builder: loading...')\n",
        "# 1 is passed so that load_data_and_labels() will parse the whole dataset\n",
        "sentences, labels = load_data_and_labels(1)\n",
        "print('vocab_builder: padding...')\n",
        "sentences_padded = pad_sentences(sentences)\n",
        "print('vocab_builder: building vocabularies...')\n",
        "vocabulary, vocabulary_inv = build_vocab(sentences_padded)\n",
        "\n",
        "print('vocab_builder: writing csv...')\n",
        "voc = csv.writer(open('vocab.csv', 'w'))\n",
        "voc_inv = csv.writer(open('vocab_inv.csv', 'w'))\n",
        "\n",
        "for key, val in vocabulary.items():\n",
        "    voc.writerow([key, val])\n",
        "for val in vocabulary_inv:\n",
        "    voc_inv.writerow([val])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnCs8-VG6VAd",
        "outputId": "89823eca-871c-4fce-b9aa-a3c0e914cb66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab_builder: loading...\n",
            "\tdata_helpers: loading positive examples...\n",
            "\tdata_helpers: [OK]\n",
            "\tdata_helpers: loading negative examples...\n",
            "\tdata_helpers: [OK]\n",
            "\tdata_helpers: cleaning strings...\n",
            "\tdata_helpers: [OK]\n",
            "\tdata_helpers: generating labels...\n",
            "\tdata_helpers: [OK]\n",
            "\tdata_helpers: concatenating labels...\n",
            "\tdata_helpers: [OK]\n",
            "vocab_builder: padding...\n",
            "vocab_builder: building vocabularies...\n",
            "vocab_builder: writing csv...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preprocessing Functions**\n"
      ],
      "metadata": {
        "id": "fNpTwSSTxyHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import sys\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "use_stemmer = False\n",
        "\n",
        "def preprocess_word(word):\n",
        "    word = word.strip('\\'\"?!,.():;')\n",
        "    word = re.sub(r'(.)\\1+', r'\\1\\1', word)\n",
        "    word = re.sub(r'(-|\\')', '', word)\n",
        "    return word\n",
        "\n",
        "\n",
        "def is_valid_word(word):\n",
        "  \n",
        "    return (re.search(r'^[a-zA-Z][a-z0-9A-Z\\._]*$', word) is not None)\n",
        "\n",
        "\n",
        "def handle_emojis(tweet):\n",
        "    # Smile -- :), : ), :-), (:, ( :, (-:, :')\n",
        "    tweet = re.sub(r'(:\\s?\\)|:-\\)|\\(\\s?:|\\(-:|:\\'\\))', ' EMO_POS ', tweet)\n",
        "    # Laugh -- :D, : D, :-D, xD, x-D, XD, X-D\n",
        "    tweet = re.sub(r'(:\\s?D|:-D|x-?D|X-?D)', ' EMO_POS ', tweet)\n",
        "    # Love -- <3, :*\n",
        "    tweet = re.sub(r'(<3|:\\*)', ' EMO_POS ', tweet)\n",
        "    # Wink -- ;-), ;), ;-D, ;D, (;,  (-;\n",
        "    tweet = re.sub(r'(;-?\\)|;-?D|\\(-?;)', ' EMO_POS ', tweet)\n",
        "    # Sad -- :-(, : (, :(, ):, )-:\n",
        "    tweet = re.sub(r'(:\\s?\\(|:-\\(|\\)\\s?:|\\)-:)', ' EMO_NEG ', tweet)\n",
        "    # Cry -- :,(, :'(, :\"(\n",
        "    tweet = re.sub(r'(:,\\(|:\\'\\(|:\"\\()', ' EMO_NEG ', tweet)\n",
        "    return tweet\n",
        "\n",
        "\n",
        "def preprocess_tweet(tweet):\n",
        "    processed_tweet = []\n",
        "    # Convert to lower case\n",
        "    tweet = tweet.lower()\n",
        "    # Replaces URLs with the word URL\n",
        "    tweet = re.sub(r'((www\\.[\\S]+)|(https?://[\\S]+))', ' URL ', tweet)\n",
        "    # Replace @handle with the word USER_MENTION\n",
        "    tweet = re.sub(r'@[\\S]+', 'USER_MENTION', tweet)\n",
        "    # Replaces #hashtag with hashtag\n",
        "    tweet = re.sub(r'#(\\S+)', r' \\1 ', tweet)\n",
        "    # Remove RT (retweet)\n",
        "    tweet = re.sub(r'\\brt\\b', '', tweet)\n",
        "    # Replace 2+ dots with space\n",
        "    tweet = re.sub(r'\\.{2,}', ' ', tweet)\n",
        "    # Strip space, \" and ' from tweet\n",
        "    tweet = tweet.strip(' \"\\'')\n",
        "    # Replace emojis with either EMO_POS or EMO_NEG\n",
        "    tweet = handle_emojis(tweet)\n",
        "    # Replace multiple spaces with a single space\n",
        "    tweet = re.sub(r'\\s+', ' ', tweet)\n",
        "    words = tweet.split()\n",
        "\n",
        "    for word in words:\n",
        "        word = preprocess_word(word)\n",
        "        if is_valid_word(word):\n",
        "            if use_stemmer:\n",
        "                word = str(porter_stemmer.stem(word))\n",
        "            processed_tweet.append(word)\n",
        "\n",
        "    return ' '.join(processed_tweet)\n",
        "\n",
        "\n",
        "def preprocess_csv(csv_file_name, processed_file_name, test_file=False):\n",
        "    save_to_file = open(processed_file_name, 'w')\n",
        "\n",
        "    with open(csv_file_name, 'r') as csv:\n",
        "        lines = csv.readlines()\n",
        "        total = len(lines)\n",
        "        for i, line in enumerate(lines):\n",
        "            tweet_id = line[:line.find(',')]\n",
        "            if not test_file:\n",
        "                line = line[1 + line.find(','):]\n",
        "                positive = int(line[:line.find(',')])\n",
        "            line = line[1 + line.find(','):]\n",
        "            tweet = line\n",
        "            processed_tweet = preprocess_tweet(tweet)\n",
        "            if not test_file:\n",
        "                save_to_file.write('%s,%d,%s\\n' %\n",
        "                                   (tweet_id, positive, processed_tweet))\n",
        "            else:\n",
        "                save_to_file.write('%s,%s\\n' %\n",
        "                                   (tweet_id, processed_tweet))\n",
        "    save_to_file.close()\n",
        "    print ('\\nSaved processed tweets to: %s' % processed_file_name)\n",
        "    return processed_file_name\n"
      ],
      "metadata": {
        "id": "XRteIBbPx9w5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PreProcessing The Training Dataset**"
      ],
      "metadata": {
        "id": "SpExQRslpzRw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    use_stemmer = False\n",
        "    csv_file_name = '/content/Elonmusktweets.csv'\n",
        "    processed_file_name = 'Elonmusktweets'  + '-processed.csv'\n",
        "    if use_stemmer:\n",
        "        porter_stemmer = PorterStemmer()\n",
        "        processed_file_name = 'Elonmusktweets' + '-processed-stemmed.csv'\n",
        "    preprocess_csv(csv_file_name, processed_file_name, test_file=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "B-Ea4V-S-VA1",
        "outputId": "c90932b6-73c9-4035-e64d-3e48742b7299"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Saved processed tweets to: Elonmusktweets-processed.csv\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Elonmusktweets-processed.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preprocessing Testing Tweets**"
      ],
      "metadata": {
        "id": "ecY7ZSYZqFjN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    use_stemmer = False\n",
        "    csv_file_name = '/content/TestMusk.csv'\n",
        "    processed_file_name = 'TestMusk'  + '-processed.csv'\n",
        "    if use_stemmer:\n",
        "        porter_stemmer = PorterStemmer()\n",
        "        processed_file_name = 'TestMusk' + '-processed-stemmed.csv'\n",
        "    preprocess_csv(csv_file_name, processed_file_name, test_file=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "QrVtiEieDAXB",
        "outputId": "6abeb4fb-0eda-48f8-a347-6668c972f5f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Saved processed tweets to: TestMusk-processed.csv\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'TestMusk-processed.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Determining Frequency Distributions of Unigrams and Bigrams**"
      ],
      "metadata": {
        "id": "pPXXuB69qSVe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Functional Block**"
      ],
      "metadata": {
        "id": "KRrbPHavqohY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import FreqDist\n",
        "import pickle\n",
        "import sys\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "# Takes in a preprocessed CSV file and gives statistics\n",
        "# Writes the frequency distribution of words and bigrams\n",
        "# to pickle files.\n",
        "\n",
        "\n",
        "def analyze_tweet(tweet):\n",
        "    result = {}\n",
        "    result['MENTIONS'] = tweet.count('USER_MENTION')\n",
        "    result['URLS'] = tweet.count('URL')\n",
        "    result['POS_EMOS'] = tweet.count('EMO_POS')\n",
        "    result['NEG_EMOS'] = tweet.count('EMO_NEG')\n",
        "    tweet = tweet.replace('USER_MENTION', '').replace(\n",
        "        'URL', '')\n",
        "    words = tweet.split()\n",
        "    result['WORDS'] = len(words)\n",
        "    bigrams = get_bigrams(words)\n",
        "    result['BIGRAMS'] = len(bigrams)\n",
        "    return result, words, bigrams\n",
        "\n",
        "\n",
        "def get_bigrams(tweet_words):\n",
        "    bigrams = []\n",
        "    num_words = len(tweet_words)\n",
        "    for i in range(num_words - 1):\n",
        "        bigrams.append((tweet_words[i], tweet_words[i + 1]))\n",
        "    return bigrams\n",
        "\n",
        "\n",
        "def get_bigram_freqdist(bigrams):\n",
        "    freq_dict = {}\n",
        "    for bigram in bigrams:\n",
        "        if freq_dict.get(bigram):\n",
        "            freq_dict[bigram] += 1\n",
        "        else:\n",
        "            freq_dict[bigram] = 1\n",
        "    counter = Counter(freq_dict)\n",
        "    return counter\n",
        "  "
      ],
      "metadata": {
        "id": "AW7E1ZZOC8O0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Statistical Analysis of the Dataset put into execution**"
      ],
      "metadata": {
        "id": "Gz_1rAFPqit7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    num_tweets, num_pos_tweets, num_neg_tweets = 0, 0, 0\n",
        "    num_mentions, max_mentions = 0, 0\n",
        "    num_emojis, num_pos_emojis, num_neg_emojis, max_emojis = 0, 0, 0, 0\n",
        "    num_urls, max_urls = 0, 0\n",
        "    num_words, num_unique_words, min_words, max_words = 0, 0, 1e6, 0\n",
        "    num_bigrams, num_unique_bigrams = 0, 0\n",
        "    all_words = []\n",
        "    all_bigrams = []\n",
        "    with open('/content/Elonmusktweets-processed.csv', 'r') as csv:\n",
        "        lines = csv.readlines()\n",
        "        num_tweets = len(lines)\n",
        "        for i, line in enumerate(lines):\n",
        "            t_id, if_pos, tweet = line.strip().split(',')\n",
        "            if_pos = int(if_pos)\n",
        "            if if_pos:\n",
        "                num_pos_tweets += 1\n",
        "            else:\n",
        "                num_neg_tweets += 1\n",
        "            result, words, bigrams = analyze_tweet(tweet)\n",
        "            num_mentions += result['MENTIONS']\n",
        "            max_mentions = max(max_mentions, result['MENTIONS'])\n",
        "            num_pos_emojis += result['POS_EMOS']\n",
        "            num_neg_emojis += result['NEG_EMOS']\n",
        "            max_emojis = max(\n",
        "                max_emojis, result['POS_EMOS'] + result['NEG_EMOS'])\n",
        "            num_urls += result['URLS']\n",
        "            max_urls = max(max_urls, result['URLS'])\n",
        "            num_words += result['WORDS']\n",
        "            min_words = min(min_words, result['WORDS'])\n",
        "            max_words = max(max_words, result['WORDS'])\n",
        "            all_words.extend(words)\n",
        "            num_bigrams += result['BIGRAMS']\n",
        "            all_bigrams.extend(bigrams)\n",
        "    num_emojis = num_pos_emojis + num_neg_emojis\n",
        "    unique_words = list(set(all_words))\n",
        "    with open('Elonmusktweets-processed' + '-unique.txt', 'w') as uwf:\n",
        "        uwf.write('\\n'.join(unique_words))\n",
        "    num_unique_words = len(unique_words)\n",
        "    num_unique_bigrams = len(set(all_bigrams))\n",
        "    print('\\nCalculating frequency distribution')\n",
        "    # Unigrams\n",
        "    freq_dist = FreqDist(all_words)\n",
        "    pkl_file_name = 'Elonmusktweets-processed' + '-freqdist.pkl'\n",
        "    with open(pkl_file_name, 'wb') as pkl_file:\n",
        "        pickle.dump(freq_dist, pkl_file)\n",
        "    print('Saved uni-frequency distribution to %s' % pkl_file_name)\n",
        "    # Bigrams\n",
        "    bigram_freq_dist = get_bigram_freqdist(all_bigrams)\n",
        "    bi_pkl_file_name = 'Elonmusktweets-processed' + '-freqdist-bi.pkl'\n",
        "    with open(bi_pkl_file_name, 'wb') as pkl_file:\n",
        "        pickle.dump(bigram_freq_dist, pkl_file)\n",
        "    print('Saved bi-frequency distribution to %s' % bi_pkl_file_name)\n",
        "    print('\\n[Analysis Statistics]')\n",
        "    print('Tweets => Total: %d, Positive: %d, Negative: %d' % (num_tweets, num_pos_tweets, num_neg_tweets))\n",
        "    print('User Mentions => Total: %d, Avg: %.4f, Max: %d' % (num_mentions, num_mentions / float(num_tweets), max_mentions))\n",
        "    print('URLs => Total: %d, Avg: %.4f, Max: %d' % (num_urls, num_urls / float(num_tweets), max_urls))\n",
        "    print('Emojis => Total: %d, Positive: %d, Negative: %d, Avg: %.4f, Max: %d' % (num_emojis, num_pos_emojis, num_neg_emojis, num_emojis / float(num_tweets), max_emojis))\n",
        "    print('Words => Total: %d, Unique: %d, Avg: %.4f, Max: %d, Min: %d' % (num_words, num_unique_words, num_words / float(num_tweets), max_words, min_words))\n",
        "    print('Bigrams => Total: %d, Unique: %d, Avg: %.4f' % (num_bigrams, num_unique_bigrams, num_bigrams / float(num_tweets)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lExQp7TUDquU",
        "outputId": "3060bdb6-2d59-425b-f9ca-8398e1026431"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Calculating frequency distribution\n",
            "Saved uni-frequency distribution to Elonmusktweets-processed-freqdist.pkl\n",
            "Saved bi-frequency distribution to Elonmusktweets-processed-freqdist-bi.pkl\n",
            "\n",
            "[Analysis Statistics]\n",
            "Tweets => Total: 958, Positive: 648, Negative: 310\n",
            "User Mentions => Total: 1279, Avg: 1.3351, Max: 11\n",
            "URLs => Total: 98, Avg: 0.1023, Max: 2\n",
            "Emojis => Total: 3, Positive: 3, Negative: 0, Avg: 0.0031, Max: 1\n",
            "Words => Total: 8185, Unique: 2276, Avg: 8.5438, Max: 46, Min: 0\n",
            "Bigrams => Total: 7396, Unique: 6014, Avg: 7.7203\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Utility Functions**"
      ],
      "metadata": {
        "id": "z9clpYnLq0-G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import pickle\n",
        "import random\n",
        "\n",
        "\n",
        "def file_to_wordset(filename):\n",
        "    ''' Converts a file with a word per line to a Python set '''\n",
        "    words = []\n",
        "    with open(filename, 'r') as f:\n",
        "        for line in f:\n",
        "            words.append(line.strip())\n",
        "    return set(words)\n",
        "\n",
        "\n",
        "def write_status(i, total):\n",
        "    ''' Writes status of a process to console '''\n",
        "    sys.stdout.write('\\r')\n",
        "    sys.stdout.write('Processing %d/%d' % (i, total))\n",
        "    sys.stdout.flush()\n",
        "\n",
        "\n",
        "def save_results_to_csv(results, csv_file):\n",
        "    ''' Save list of type [(tweet_id, positive)] to csv in Kaggle format '''\n",
        "    with open(csv_file, 'w') as csv:\n",
        "        csv.write('id,prediction\\n')\n",
        "        for tweet_id, pred in results:\n",
        "            csv.write(tweet_id)\n",
        "            csv.write(',')\n",
        "            csv.write(str(pred))\n",
        "            csv.write('\\n')\n",
        "\n",
        "\n",
        "def top_n_words(pkl_file_name, N, shift=0):\n",
        "    \"\"\"\n",
        "    Returns a dictionary of form {word:rank} of top N words from a pickle\n",
        "    file which has a nltk FreqDist object generated by stats.py\n",
        "    Args:\n",
        "        pkl_file_name (str): Name of pickle file\n",
        "        N (int): The number of words to get\n",
        "        shift: amount to shift the rank from 0.\n",
        "    Returns:\n",
        "        dict: Of form {word:rank}\n",
        "    \"\"\"\n",
        "    with open(pkl_file_name, 'rb') as pkl_file:\n",
        "        freq_dist = pickle.load(pkl_file)\n",
        "    most_common = freq_dist.most_common(N)\n",
        "    words = {p[0]: i + shift for i, p in enumerate(most_common)}\n",
        "    return words\n",
        "\n",
        "\n",
        "def top_n_bigrams(pkl_file_name, N, shift=0):\n",
        "    \"\"\"\n",
        "    Returns a dictionary of form {bigram:rank} of top N bigrams from a pickle\n",
        "    file which has a Counter object generated by stats.py\n",
        "    Args:\n",
        "        pkl_file_name (str): Name of pickle file\n",
        "        N (int): The number of bigrams to get\n",
        "        shift: amount to shift the rank from 0.\n",
        "    Returns:\n",
        "        dict: Of form {bigram:rank}\n",
        "    \"\"\"\n",
        "    with open(pkl_file_name, 'rb') as pkl_file:\n",
        "        freq_dist = pickle.load(pkl_file)\n",
        "    most_common = freq_dist.most_common(N)\n",
        "    bigrams = {p[0]: i for i, p in enumerate(most_common)}\n",
        "    return bigrams\n",
        "\n",
        "\n",
        "def split_data(tweets, validation_split=0.1):\n",
        "    \"\"\"Split the data into training and validation sets\n",
        "    Args:\n",
        "        tweets (list): list of tuples\n",
        "        validation_split (float, optional): validation split %\n",
        "    Returns:\n",
        "        (list, list): training-set, validation-set\n",
        "    \"\"\"\n",
        "    index = int((1 - validation_split) * len(tweets))\n",
        "    random.shuffle(tweets)\n",
        "    return tweets[:index], tweets[index:]"
      ],
      "metadata": {
        "id": "WPgU7nNhJ45P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir datasets\n",
        "!mkdir models\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJcLYj12KUcb",
        "outputId": "8d8befe6-de0e-41ce-b27b-6270959cdea8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‚Äòmodels‚Äô: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Maximum Entropy Logistic Model**"
      ],
      "metadata": {
        "id": "lB2nWXsOryjM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense\n",
        "import sys\n",
        "import utils\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Performs classification using Logistic Regression.\n",
        "\n",
        "FREQ_DIST_FILE = '/content/Elonmusktweets-processed-freqdist.pkl'\n",
        "BI_FREQ_DIST_FILE = '/content/Elonmusktweets-processed-freqdist-bi.pkl'\n",
        "TRAIN_PROCESSED_FILE = '/content/Elonmusktweets-processed.csv'\n",
        "TEST_PROCESSED_FILE = '/content/TestMusk-processed.csv'\n",
        "TRAIN = True\n",
        "UNIGRAM_SIZE = 15000\n",
        "VOCAB_SIZE = UNIGRAM_SIZE\n",
        "USE_BIGRAMS = True\n",
        "if USE_BIGRAMS:\n",
        "    BIGRAM_SIZE = 10000\n",
        "    VOCAB_SIZE = UNIGRAM_SIZE + BIGRAM_SIZE\n",
        "FEAT_TYPE = 'frequency'\n",
        "\n",
        "\n",
        "def get_feature_vector(tweet):\n",
        "    uni_feature_vector = []\n",
        "    bi_feature_vector = []\n",
        "    words = tweet.split()\n",
        "    for i in range(len(words) - 1):\n",
        "        word = words[i]\n",
        "        next_word = words[i + 1]\n",
        "        if unigrams.get(word):\n",
        "            uni_feature_vector.append(word)\n",
        "        if USE_BIGRAMS:\n",
        "            if bigrams.get((word, next_word)):\n",
        "                bi_feature_vector.append((word, next_word))\n",
        "    if len(words) >= 1:\n",
        "        if unigrams.get(words[-1]):\n",
        "            uni_feature_vector.append(words[-1])\n",
        "    return uni_feature_vector, bi_feature_vector\n",
        "\n",
        "\n",
        "def extract_features(tweets, batch_size=500, test_file=True, feat_type='presence'):\n",
        "    num_batches = int(np.ceil(len(tweets) / float(batch_size)))\n",
        "    for i in range(num_batches):\n",
        "        batch = tweets[i * batch_size: (i + 1) * batch_size]\n",
        "        features = np.zeros((batch_size, VOCAB_SIZE))\n",
        "        labels = np.zeros(batch_size)\n",
        "        for j, tweet in enumerate(batch):\n",
        "            if test_file:\n",
        "                tweet_words = tweet[1][0]\n",
        "                tweet_bigrams = tweet[1][1]\n",
        "            else:\n",
        "                tweet_words = tweet[2][0]\n",
        "                tweet_bigrams = tweet[2][1]\n",
        "                labels[j] = tweet[1]\n",
        "            if feat_type == 'presence':\n",
        "                tweet_words = set(tweet_words)\n",
        "                tweet_bigrams = set(tweet_bigrams)\n",
        "            for word in tweet_words:\n",
        "                idx = unigrams.get(word)\n",
        "                if idx:\n",
        "                    features[j, idx] += 1\n",
        "            if USE_BIGRAMS:\n",
        "                for bigram in tweet_bigrams:\n",
        "                    idx = bigrams.get(bigram)\n",
        "                    if idx:\n",
        "                        features[j, UNIGRAM_SIZE + idx] += 1\n",
        "        yield features, labels\n",
        "\n",
        "\n",
        "def process_tweets(csv_file, test_file=True):\n",
        "    tweets = []\n",
        "    print('Generating feature vectors')\n",
        "    with open(csv_file, 'r') as csv:\n",
        "        lines = csv.readlines()\n",
        "        total = len(lines)\n",
        "        for i, line in enumerate(lines):\n",
        "            if test_file:\n",
        "                tweet_id, tweet = line.split(',')\n",
        "            else:\n",
        "                tweet_id, sentiment, tweet = line.split(',')\n",
        "            feature_vector = get_feature_vector(tweet)\n",
        "            if test_file:\n",
        "                tweets.append((tweet_id, feature_vector))\n",
        "            else:\n",
        "                tweets.append((tweet_id, int(sentiment), feature_vector))\n",
        "    print('\\n')\n",
        "    return tweets\n",
        "\n",
        "\n",
        "def build_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(1, input_dim=VOCAB_SIZE, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "def evaluate_model(model, val_tweets):\n",
        "    correct, total = 0, len(val_tweets)\n",
        "    for val_set_X, val_set_y in extract_features(val_tweets, feat_type=FEAT_TYPE, test_file=False):\n",
        "        prediction = model.predict_on_batch(val_set_X)\n",
        "        prediction = np.round(prediction)\n",
        "        correct += np.sum(prediction == val_set_y[:, None])\n",
        "    return float(correct) / total\n"
      ],
      "metadata": {
        "id": "vKS-rT0nr8-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "    np.random.seed(1337)\n",
        "    unigrams = top_n_words(FREQ_DIST_FILE, UNIGRAM_SIZE)\n",
        "    if USE_BIGRAMS:\n",
        "        bigrams = top_n_bigrams(BI_FREQ_DIST_FILE, BIGRAM_SIZE)\n",
        "    tweets = process_tweets(TRAIN_PROCESSED_FILE, test_file=False)\n",
        "    if TRAIN:\n",
        "        train_tweets, val_tweets = split_data(tweets)\n",
        "    else:\n",
        "        random.shuffle(tweets)\n",
        "        train_tweets = tweets\n",
        "    del tweets\n",
        "    print('Extracting features & training batches')\n",
        "    nb_epochs = 20\n",
        "    batch_size = 500\n",
        "    model = build_model()\n",
        "    n_train_batches = int(np.ceil(len(train_tweets) / float(batch_size)))\n",
        "    best_val_acc = 0.0\n",
        "    for j in range(nb_epochs):\n",
        "        i = 1\n",
        "        for training_set_X, training_set_y in extract_features(train_tweets, feat_type=FEAT_TYPE, batch_size=batch_size, test_file=False):\n",
        "            o = model.train_on_batch(training_set_X, training_set_y)\n",
        "            sys.stdout.write('\\rIteration %d/%d, loss:%.4f, acc:%.4f' %\n",
        "                             (i, n_train_batches, o[0], o[1]))\n",
        "            sys.stdout.flush()\n",
        "            i += 1\n",
        "        val_acc = evaluate_model(model, val_tweets)\n",
        "        print('\\nEpoch: %d, val_acc:%.4f' % (j + 1, val_acc))\n",
        "        random.shuffle(train_tweets)\n",
        "        if val_acc > best_val_acc:\n",
        "            print('Accuracy improved from %.4f to %.4f, saving model' % (best_val_acc, val_acc))\n",
        "            best_val_acc = val_acc\n",
        "            model.save('best_model.h5')\n",
        "    print('Testing')\n",
        "    del train_tweets\n",
        "    del model\n",
        "    model = load_model('best_model.h5')\n",
        "    test_tweets = process_tweets(TEST_PROCESSED_FILE, test_file=True)\n",
        "    n_test_batches = int(np.ceil(len(test_tweets) / float(batch_size)))\n",
        "    predictions = np.array([])\n",
        "    print('Predicting batches')\n",
        "    i = 1\n",
        "    for test_set_X, _ in extract_features(test_tweets, feat_type=FEAT_TYPE, batch_size=batch_size, test_file=True):\n",
        "        prediction = np.round(model.predict_on_batch(test_set_X).flatten())\n",
        "        predictions = np.concatenate((predictions, prediction))\n",
        "        i += 1\n",
        "    predictions = [(str(j), int(predictions[j]))\n",
        "                   for j in range(len(test_tweets))]\n",
        "    save_results_to_csv(predictions, 'logistic.csv')\n",
        "    print('\\nSaved to logistic.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHaAESDUsShk",
        "outputId": "7df2a355-cc50-4f1d-8449-63def18d1a4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating feature vectors\n",
            "\n",
            "\n",
            "Extracting features & training batches\n",
            "Iteration 2/2, loss:0.6940, acc:0.3260\n",
            "Epoch: 1, val_acc:0.5938\n",
            "Accuracy improved from 0.0000 to 0.5938, saving model\n",
            "Iteration 1/2, loss:0.6877, acc:0.5700"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 2/2, loss:0.6866, acc:0.4640\n",
            "Epoch: 2, val_acc:0.6250\n",
            "Accuracy improved from 0.5938 to 0.6250, saving model\n",
            "Iteration 2/2, loss:0.6802, acc:0.5240\n",
            "Epoch: 3, val_acc:0.6146\n",
            "Iteration 2/2, loss:0.6752, acc:0.5320\n",
            "Epoch: 4, val_acc:0.6250\n",
            "Iteration 2/2, loss:0.6695, acc:0.5800\n",
            "Epoch: 5, val_acc:0.6458\n",
            "Accuracy improved from 0.6250 to 0.6458, saving model\n",
            "Iteration 2/2, loss:0.6638, acc:0.5980\n",
            "Epoch: 6, val_acc:0.6667\n",
            "Accuracy improved from 0.6458 to 0.6667, saving model\n",
            "Iteration 2/2, loss:0.6594, acc:0.5920\n",
            "Epoch: 7, val_acc:0.6667\n",
            "Iteration 2/2, loss:0.6562, acc:0.5920\n",
            "Epoch: 8, val_acc:0.6667\n",
            "Iteration 2/2, loss:0.6486, acc:0.6060\n",
            "Epoch: 9, val_acc:0.6667\n",
            "Iteration 2/2, loss:0.6423, acc:0.6120\n",
            "Epoch: 10, val_acc:0.6771\n",
            "Accuracy improved from 0.6667 to 0.6771, saving model\n",
            "Iteration 2/2, loss:0.6407, acc:0.5980\n",
            "Epoch: 11, val_acc:0.6875\n",
            "Accuracy improved from 0.6771 to 0.6875, saving model\n",
            "Iteration 2/2, loss:0.6380, acc:0.6100\n",
            "Epoch: 12, val_acc:0.6875\n",
            "Iteration 2/2, loss:0.6292, acc:0.6060\n",
            "Epoch: 13, val_acc:0.6667\n",
            "Iteration 2/2, loss:0.6215, acc:0.6300\n",
            "Epoch: 14, val_acc:0.6667\n",
            "Iteration 2/2, loss:0.6236, acc:0.5920\n",
            "Epoch: 15, val_acc:0.6667\n",
            "Iteration 2/2, loss:0.6249, acc:0.5920\n",
            "Epoch: 16, val_acc:0.6667\n",
            "Iteration 2/2, loss:0.6085, acc:0.6140\n",
            "Epoch: 17, val_acc:0.6667\n",
            "Iteration 2/2, loss:0.6028, acc:0.6180\n",
            "Epoch: 18, val_acc:0.6667\n",
            "Iteration 2/2, loss:0.6017, acc:0.6200\n",
            "Epoch: 19, val_acc:0.6667\n",
            "Iteration 2/2, loss:0.6001, acc:0.6220"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/init_ops.py:93: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 20, val_acc:0.6667\n",
            "Testing\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/init_ops.py:93: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating feature vectors\n",
            "\n",
            "\n",
            "Predicting batches\n",
            "\n",
            "Saved to logistic.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Decision Tree**"
      ],
      "metadata": {
        "id": "PFmX3WUvuuIp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import utils\n",
        "import random\n",
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from scipy.sparse import lil_matrix\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "# Performs classification using Decision Tree.\n",
        "\n",
        "FREQ_DIST_FILE = '/content/Elonmusktweets-processed-freqdist.pkl'\n",
        "BI_FREQ_DIST_FILE = '/content/Elonmusktweets-processed-freqdist-bi.pkl'\n",
        "TRAIN_PROCESSED_FILE = '/content/Elonmusktweets-processed.csv'\n",
        "TEST_PROCESSED_FILE = '/content/TestMusk-processed.csv'\n",
        "\n",
        "# True while training.\n",
        "TRAIN = True\n",
        "UNIGRAM_SIZE = 15000\n",
        "VOCAB_SIZE = UNIGRAM_SIZE\n",
        "\n",
        "# If using bigrams.\n",
        "USE_BIGRAMS = False\n",
        "if USE_BIGRAMS:\n",
        "    BIGRAM_SIZE = 10000\n",
        "    VOCAB_SIZE = UNIGRAM_SIZE + BIGRAM_SIZE\n",
        "FEAT_TYPE = 'frequency'\n",
        "\n",
        "\n",
        "def get_feature_vector(tweet):\n",
        "    uni_feature_vector = []\n",
        "    bi_feature_vector = []\n",
        "    words = tweet.split()\n",
        "    for i in range(len(words) - 1):\n",
        "        word = words[i]\n",
        "        next_word = words[i + 1]\n",
        "        if unigrams.get(word):\n",
        "            uni_feature_vector.append(word)\n",
        "        if USE_BIGRAMS:\n",
        "            if bigrams.get((word, next_word)):\n",
        "                bi_feature_vector.append((word, next_word))\n",
        "    if len(words) >= 1:\n",
        "        if unigrams.get(words[-1]):\n",
        "            uni_feature_vector.append(words[-1])\n",
        "    return uni_feature_vector, bi_feature_vector\n",
        "\n",
        "\n",
        "def extract_features(tweets, batch_size=500, test_file=True, feat_type='presence'):\n",
        "    num_batches = int(np.ceil(len(tweets) / float(batch_size)))\n",
        "    for i in range(num_batches):\n",
        "        batch = tweets[i * batch_size: (i + 1) * batch_size]\n",
        "        features = lil_matrix((batch_size, VOCAB_SIZE))\n",
        "        labels = np.zeros(batch_size)\n",
        "        for j, tweet in enumerate(batch):\n",
        "            if test_file:\n",
        "                tweet_words = tweet[1][0]\n",
        "                tweet_bigrams = tweet[1][1]\n",
        "            else:\n",
        "                tweet_words = tweet[2][0]\n",
        "                tweet_bigrams = tweet[2][1]\n",
        "                labels[j] = tweet[1]\n",
        "            if feat_type == 'presence':\n",
        "                tweet_words = set(tweet_words)\n",
        "                tweet_bigrams = set(tweet_bigrams)\n",
        "            for word in tweet_words:\n",
        "                idx = unigrams.get(word)\n",
        "                if idx:\n",
        "                    features[j, idx] += 1\n",
        "            if USE_BIGRAMS:\n",
        "                for bigram in tweet_bigrams:\n",
        "                    idx = bigrams.get(bigram)\n",
        "                    if idx:\n",
        "                        features[j, UNIGRAM_SIZE + idx] += 1\n",
        "        yield features, labels\n",
        "\n",
        "\n",
        "def apply_tf_idf(X):\n",
        "    \"\"\"\n",
        "    Fits X for TF-IDF vectorization and returns the transformer.\n",
        "    \"\"\"\n",
        "    transformer = TfidfTransformer(smooth_idf=True, sublinear_tf=True, use_idf=True)\n",
        "    transformer.fit(X)\n",
        "    return transformer\n",
        "\n",
        "\n",
        "def process_tweets(csv_file, test_file=True):\n",
        "    \"\"\"Returns a list of tuples of type (tweet_id, feature_vector)\n",
        "            or (tweet_id, sentiment, feature_vector)\n",
        "    Args:\n",
        "        csv_file (str): Name of processed csv file generated by preprocess.py\n",
        "        test_file (bool, optional): If processing test file\n",
        "    Returns:\n",
        "        list: Of tuples\n",
        "    \"\"\"\n",
        "    tweets = []\n",
        "    print('Generating feature vectors')\n",
        "    with open(csv_file, 'r') as csv:\n",
        "        lines = csv.readlines()\n",
        "        total = len(lines)\n",
        "        for i, line in enumerate(lines):\n",
        "            if test_file:\n",
        "                tweet_id, tweet = line.split(',')\n",
        "            else:\n",
        "                tweet_id, sentiment, tweet = line.split(',')\n",
        "            feature_vector = get_feature_vector(tweet)\n",
        "            if test_file:\n",
        "                tweets.append((tweet_id, feature_vector))\n",
        "            else:\n",
        "                tweets.append((tweet_id, int(sentiment), feature_vector))\n",
        "    print ('\\n')\n",
        "    return tweets"
      ],
      "metadata": {
        "id": "QbOQo0KhvU9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    TRAIN = False\n",
        "    np.random.seed(1337)\n",
        "    unigrams = top_n_words(FREQ_DIST_FILE, UNIGRAM_SIZE)\n",
        "    if USE_BIGRAMS:\n",
        "        bigrams = top_n_bigrams(BI_FREQ_DIST_FILE, BIGRAM_SIZE)\n",
        "    tweets = process_tweets(TRAIN_PROCESSED_FILE, test_file=False)\n",
        "    if TRAIN:\n",
        "        train_tweets, val_tweets = split_data(tweets)\n",
        "    else:\n",
        "        random.shuffle(tweets)\n",
        "        train_tweets = tweets\n",
        "    del tweets\n",
        "    print('Extracting features & training batches')\n",
        "    clf = DecisionTreeClassifier(max_depth=25)\n",
        "    batch_size = len(train_tweets)\n",
        "    i = 1\n",
        "    n_train_batches = int(np.ceil(len(train_tweets) / float(batch_size)))\n",
        "    for training_set_X, training_set_y in extract_features(train_tweets, test_file=False, feat_type=FEAT_TYPE, batch_size=batch_size):\n",
        "\n",
        "        i += 1\n",
        "        if FEAT_TYPE == 'frequency':\n",
        "            tfidf = apply_tf_idf(training_set_X)\n",
        "            training_set_X = tfidf.transform(training_set_X)\n",
        "        clf.fit(training_set_X, training_set_y)\n",
        "    print('\\n')\n",
        "    print('Testing')\n",
        "    if TRAIN:\n",
        "        correct, total = 0, len(val_tweets)\n",
        "        i = 1\n",
        "        batch_size = len(val_tweets)\n",
        "        n_val_batches = int(np.ceil(len(val_tweets) / float(batch_size)))\n",
        "        for val_set_X, val_set_y in extract_features(val_tweets, test_file=False, feat_type=FEAT_TYPE, batch_size=batch_size):\n",
        "            if FEAT_TYPE == 'frequency':\n",
        "                val_set_X = tfidf.transform(val_set_X)\n",
        "            prediction = clf.predict(val_set_X)\n",
        "            correct += np.sum(prediction == val_set_y)\n",
        "            i += 1\n",
        "        print('\\nCorrect: %d/%d = %.4f %%' % (correct, total, correct * 100. / total))\n",
        "    else:\n",
        "        del train_tweets\n",
        "        test_tweets = process_tweets(TEST_PROCESSED_FILE, test_file=True)\n",
        "        n_test_batches = int(np.ceil(len(test_tweets) / float(batch_size)))\n",
        "        predictions = np.array([])\n",
        "        print('Predicting batches')\n",
        "        i = 1\n",
        "        for test_set_X, _ in extract_features(test_tweets, test_file=True, feat_type=FEAT_TYPE):\n",
        "            if FEAT_TYPE == 'frequency':\n",
        "                test_set_X = tfidf.transform(test_set_X)\n",
        "            prediction = clf.predict(test_set_X)\n",
        "            predictions = np.concatenate((predictions, prediction))\n",
        "            i += 1\n",
        "        predictions = [(str(j), int(predictions[j]))\n",
        "                       for j in range(len(test_tweets))]\n",
        "        save_results_to_csv(predictions, 'decisiontree.csv')\n",
        "        print('\\nSaved to decisiontree.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4dFIZAqwCwF",
        "outputId": "14ca6bb4-be86-442a-9830-e9bc5ed8714b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating feature vectors\n",
            "\n",
            "\n",
            "Extracting features & training batches\n",
            "\n",
            "\n",
            "Testing\n",
            "Generating feature vectors\n",
            "\n",
            "\n",
            "Predicting batches\n",
            "\n",
            "Saved to decisiontree.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9HK7N-7NxfZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SVM Analysis**"
      ],
      "metadata": {
        "id": "16uZzlIoxgIk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import svm\n",
        "import utils\n",
        "import random\n",
        "import numpy as np\n",
        "from scipy.sparse import lil_matrix\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "# Performs classification using SVM.\n",
        "\n",
        "FREQ_DIST_FILE = '/content/Elonmusktweets-processed-freqdist-bi.pkl'\n",
        "BI_FREQ_DIST_FILE = '/content/Elonmusktweets-processed-freqdist-bi.pkl'\n",
        "TRAIN_PROCESSED_FILE = '/content/Elonmusktweets-processed.csv'\n",
        "TEST_PROCESSED_FILE = '/content/TestMusk-processed.csv'\n",
        "TRAIN = True\n",
        "UNIGRAM_SIZE = 15000\n",
        "VOCAB_SIZE = UNIGRAM_SIZE\n",
        "USE_BIGRAMS = True\n",
        "if USE_BIGRAMS:\n",
        "    BIGRAM_SIZE = 10000\n",
        "    VOCAB_SIZE = UNIGRAM_SIZE + BIGRAM_SIZE\n",
        "FEAT_TYPE = 'frequency'\n",
        "\n",
        "\n",
        "def get_feature_vector(tweet):\n",
        "    uni_feature_vector = []\n",
        "    bi_feature_vector = []\n",
        "    words = tweet.split()\n",
        "    for i in range(len(words) - 1):\n",
        "        word = words[i]\n",
        "        next_word = words[i + 1]\n",
        "        if unigrams.get(word):\n",
        "            uni_feature_vector.append(word)\n",
        "        if USE_BIGRAMS:\n",
        "            if bigrams.get((word, next_word)):\n",
        "                bi_feature_vector.append((word, next_word))\n",
        "    if len(words) >= 1:\n",
        "        if unigrams.get(words[-1]):\n",
        "            uni_feature_vector.append(words[-1])\n",
        "    return uni_feature_vector, bi_feature_vector\n",
        "\n",
        "\n",
        "def extract_features(tweets, batch_size=500, test_file=True, feat_type='presence'):\n",
        "    num_batches = int(np.ceil(len(tweets) / float(batch_size)))\n",
        "    for i in range(num_batches):\n",
        "        batch = tweets[i * batch_size: (i + 1) * batch_size]\n",
        "        features = lil_matrix((batch_size, VOCAB_SIZE))\n",
        "        labels = np.zeros(batch_size)\n",
        "        for j, tweet in enumerate(batch):\n",
        "            if test_file:\n",
        "                tweet_words = tweet[1][0]\n",
        "                tweet_bigrams = tweet[1][1]\n",
        "            else:\n",
        "                tweet_words = tweet[2][0]\n",
        "                tweet_bigrams = tweet[2][1]\n",
        "                labels[j] = tweet[1]\n",
        "            if feat_type == 'presence':\n",
        "                tweet_words = set(tweet_words)\n",
        "                tweet_bigrams = set(tweet_bigrams)\n",
        "            for word in tweet_words:\n",
        "                idx = unigrams.get(word)\n",
        "                if idx:\n",
        "                    features[j, idx] += 1\n",
        "            if USE_BIGRAMS:\n",
        "                for bigram in tweet_bigrams:\n",
        "                    idx = bigrams.get(bigram)\n",
        "                    if idx:\n",
        "                        features[j, UNIGRAM_SIZE + idx] += 1\n",
        "        yield features, labels\n",
        "\n",
        "\n",
        "def apply_tf_idf(X):\n",
        "    transformer = TfidfTransformer(smooth_idf=True, sublinear_tf=True, use_idf=True)\n",
        "    transformer.fit(X)\n",
        "    return transformer\n",
        "\n",
        "\n",
        "def process_tweets(csv_file, test_file=True):\n",
        "    \"\"\"Returns a list of tuples of type (tweet_id, feature_vector)\n",
        "            or (tweet_id, sentiment, feature_vector)\n",
        "    Args:\n",
        "        csv_file (str): Name of processed csv file generated by preprocess.py\n",
        "        test_file (bool, optional): If processing test file\n",
        "    Returns:\n",
        "        list: Of tuples\n",
        "    \"\"\"\n",
        "    tweets = []\n",
        "    print('Generating feature vectors')\n",
        "    with open(csv_file, 'r') as csv:\n",
        "        lines = csv.readlines()\n",
        "        total = len(lines)\n",
        "        for i, line in enumerate(lines):\n",
        "            if test_file:\n",
        "                tweet_id, tweet = line.split(',')\n",
        "            else:\n",
        "                tweet_id, sentiment, tweet = line.split(',')\n",
        "            feature_vector = get_feature_vector(tweet)\n",
        "            if test_file:\n",
        "                tweets.append((tweet_id, feature_vector))\n",
        "            else:\n",
        "                tweets.append((tweet_id, int(sentiment), feature_vector))\n",
        "    print('\\n')\n",
        "    return tweets\n"
      ],
      "metadata": {
        "id": "JHr_N2g7xqHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    TRAIN = False\n",
        "    np.random.seed(1337)\n",
        "    unigrams = top_n_words(FREQ_DIST_FILE, UNIGRAM_SIZE)\n",
        "    if USE_BIGRAMS:\n",
        "        bigrams = top_n_bigrams(BI_FREQ_DIST_FILE, BIGRAM_SIZE)\n",
        "    tweets = process_tweets(TRAIN_PROCESSED_FILE, test_file=False)\n",
        "    if TRAIN:\n",
        "        train_tweets, val_tweets = split_data(tweets)\n",
        "    else:\n",
        "        random.shuffle(tweets)\n",
        "        train_tweets = tweets\n",
        "    del tweets\n",
        "    print('Extracting features & training batches')\n",
        "    clf = svm.LinearSVC(C=0.1)\n",
        "    batch_size = len(train_tweets)\n",
        "    i = 1\n",
        "    n_train_batches = int(np.ceil(len(train_tweets) / float(batch_size)))\n",
        "    for training_set_X, training_set_y in extract_features(train_tweets, test_file=False, feat_type=FEAT_TYPE, batch_size=batch_size):\n",
        "        i += 1\n",
        "        if FEAT_TYPE == 'frequency':\n",
        "            tfidf = apply_tf_idf(training_set_X)\n",
        "            training_set_X = tfidf.transform(training_set_X)\n",
        "        clf.fit(training_set_X, training_set_y)\n",
        "    print('\\n')\n",
        "    print('Testing')\n",
        "    if TRAIN:\n",
        "        correct, total = 0, len(val_tweets)\n",
        "        i = 1\n",
        "        batch_size = len(val_tweets)\n",
        "        n_val_batches = int(np.ceil(len(val_tweets) / float(batch_size)))\n",
        "        for val_set_X, val_set_y in extract_features(val_tweets, test_file=False, feat_type=FEAT_TYPE, batch_size=batch_size):\n",
        "            if FEAT_TYPE == 'frequency':\n",
        "                val_set_X = tfidf.transform(val_set_X)\n",
        "            prediction = clf.predict(val_set_X)\n",
        "            correct += np.sum(prediction == val_set_y)\n",
        "            i += 1\n",
        "        print('\\nCorrect: %d/%d = %.4f %%' % (correct, total, correct * 100. / total))\n",
        "    else:\n",
        "        del train_tweets\n",
        "        test_tweets = process_tweets(TEST_PROCESSED_FILE, test_file=True)\n",
        "        n_test_batches = int(np.ceil(len(test_tweets) / float(batch_size)))\n",
        "        predictions = np.array([])\n",
        "        print('Predicting batches')\n",
        "        i = 1\n",
        "        for test_set_X, _ in extract_features(test_tweets, test_file=True, feat_type=FEAT_TYPE):\n",
        "            if FEAT_TYPE == 'frequency':\n",
        "                test_set_X = tfidf.transform(test_set_X)\n",
        "            prediction = clf.predict(test_set_X)\n",
        "            predictions = np.concatenate((predictions, prediction))\n",
        "            i += 1\n",
        "        predictions = [(str(j), int(predictions[j]))\n",
        "                       for j in range(len(test_tweets))]\n",
        "        save_results_to_csv(predictions, 'svm.csv')\n",
        "        print('\\nSaved to svm.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8a1TfJytxte1",
        "outputId": "20af2985-91ad-47cf-de3e-a87a33830661"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating feature vectors\n",
            "\n",
            "\n",
            "Extracting features & training batches\n",
            "\n",
            "\n",
            "Testing\n",
            "Generating feature vectors\n",
            "\n",
            "\n",
            "Predicting batches\n",
            "\n",
            "Saved to svm.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/out.zip /content/content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OIBS75OU6AwL",
        "outputId": "686f6cb3-8a0a-46da-e5d9-ec3c8418bb23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scanning files \n",
            "  adding: content/content/ (stored 0%)\n",
            "  adding: content/content/Elonmusktweets-processed-unique.txt (deflated 51%)\n",
            "  adding: content/content/logistic.csv (deflated 61%)\n",
            "  adding: content/content/TestMusk.csv (deflated 52%)\n",
            "  adding: content/content/models/ (stored 0%)\n",
            "  adding: content/content/TestMusk-processed.csv (deflated 61%)\n",
            "  adding: content/content/svm.csv (deflated 62%)\n",
            "  adding: content/content/decisiontree.csv (deflated 61%)\n",
            "  adding: content/content/best_model.h5 (deflated 64%)\n",
            "  adding: content/content/Elonmusktweets-processed.csv (deflated 67%)\n",
            "  adding: content/content/Elonmusktweets-processed-freqdist-bi.pkl (deflated 60%)\n",
            "  adding: content/content/datasets/ (stored 0%)\n",
            "  adding: content/content/datasets/positive-words.txt (deflated 65%)\n",
            "  adding: content/content/datasets/negative-words.txt (deflated 66%)\n",
            "  adding: content/content/Elonmusktweets-processed-freqdist.pkl (deflated 58%)\n",
            "  adding: content/content/vocab.csv (deflated 51%)\n",
            "  adding: content/content/tw-data.pos (deflated 61%)\n",
            "  adding: content/content/output/ (stored 0%)\n",
            "  adding: content/content/output/run20221118-111312/ (stored 0%)\n",
            "  adding: content/content/output/run20221118-111312/log.log (deflated 70%)\n",
            "  adding: content/content/output/run20221118-112102/ (stored 0%)\n",
            "  adding: content/content/output/run20221118-112102/log.log (deflated 70%)\n",
            "  adding: content/content/output/run20221118-112024/ (stored 0%)\n",
            "  adding: content/content/output/run20221118-112024/log.log (deflated 70%)\n",
            "  adding: content/content/output/run20221118-111414/ (stored 0%)\n",
            "  adding: content/content/output/run20221118-111414/log.log (deflated 70%)\n",
            "  adding: content/content/output/run20221118-123149/ (stored 0%)\n",
            "  adding: content/content/output/run20221118-123149/log.log (deflated 70%)\n",
            "  adding: content/content/output/run20221118-123223/ (stored 0%)\n",
            "  adding: content/content/output/run20221118-123223/log.log (deflated 70%)\n",
            "  adding: content/content/output/run20221118-121406/ (stored 0%)\n",
            "  adding: content/content/output/run20221118-121406/log.log (deflated 70%)\n",
            "  adding: content/content/output/run20221118-114417/ (stored 0%)\n",
            "  adding: content/content/output/run20221118-114417/log.log (deflated 70%)\n",
            "  adding: content/content/output/run20221118-121401/ (stored 0%)\n",
            "  adding: content/content/output/run20221118-121401/log.log (deflated 70%)\n",
            "  adding: content/content/output/run20221118-112756/ (stored 0%)\n",
            "  adding: content/content/output/run20221118-112756/log.log (deflated 70%)\n",
            "  adding: content/content/output/run20221118-120754/ (stored 0%)\n",
            "  adding: content/content/output/run20221118-120754/log.log (deflated 70%)\n",
            "  adding: content/content/output/run20221118-114746/ (stored 0%)\n",
            "  adding: content/content/output/run20221118-114746/log.log (deflated 70%)\n",
            "  adding: content/content/output/run20221118-121222/ (stored 0%)\n",
            "  adding: content/content/output/run20221118-121222/log.log (deflated 70%)\n",
            "  adding: content/content/output/run20221118-121207/ (stored 0%)\n",
            "  adding: content/content/output/run20221118-121207/log.log (deflated 70%)\n",
            "  adding: content/content/output/run20221118-115841/ (stored 0%)\n",
            "  adding: content/content/output/run20221118-115841/log.log (deflated 70%)\n",
            "  adding: content/content/output/run20221118-123127/ (stored 0%)\n",
            "  adding: content/content/output/run20221118-123127/log.log (deflated 70%)\n",
            "  adding: content/content/output/run20221118-111036/ (stored 0%)\n",
            "  adding: content/content/output/run20221118-111036/log.log (deflated 70%)\n",
            "  adding: content/content/output/run20221118-121236/ (stored 0%)\n",
            "  adding: content/content/output/run20221118-121236/log.log (deflated 70%)\n",
            "  adding: content/content/output/run20221118-123348/ (stored 0%)\n",
            "  adding: content/content/output/run20221118-123348/log.log (deflated 70%)\n",
            "  adding: content/content/output/run20221118-121240/ (stored 0%)\n",
            "  adding: content/content/output/run20221118-121240/log.log (deflated 70%)\n",
            "  adding: content/content/output/run20221118-114137/ (stored 0%)\n",
            "  adding: content/content/output/run20221118-114137/log.log (deflated 70%)\n",
            "  adding: content/content/output/run20221118-121359/ (stored 0%)\n",
            "  adding: content/content/output/run20221118-121359/log.log (deflated 70%)\n",
            "  adding: content/content/output/run20221118-120245/ (stored 0%)\n",
            "  adding: content/content/output/run20221118-120245/log.log (deflated 70%)\n",
            "  adding: content/content/output/run20221118-123422/ (stored 0%)\n",
            "  adding: content/content/output/run20221118-123422/log.log (stored 0%)\n",
            "  adding: content/content/output/run20221118-120923/ (stored 0%)\n",
            "  adding: content/content/output/run20221118-120923/log.log (deflated 70%)\n",
            "  adding: content/content/output/run20221118-121004/ (stored 0%)\n",
            "  adding: content/content/output/run20221118-121004/log.log (deflated 70%)\n",
            "  adding: content/content/output/run20221118-113238/ (stored 0%)\n",
            "  adding: content/content/output/run20221118-113238/log.log (deflated 70%)\n",
            "  adding: content/content/output/run20221118-121226/ (stored 0%)\n",
            "  adding: content/content/output/run20221118-121226/log.log (deflated 70%)\n",
            "  adding: content/content/output/run20221118-121357/ (stored 0%)\n",
            "  adding: content/content/output/run20221118-121357/log.log (deflated 70%)\n",
            "  adding: content/content/output/run20221118-114249/ (stored 0%)\n",
            "  adding: content/content/output/run20221118-114249/log.log (deflated 70%)\n",
            "  adding: content/content/output/run20221118-115905/ (stored 0%)\n",
            "  adding: content/content/output/run20221118-115905/log.log (deflated 70%)\n",
            "  adding: content/content/output/run20221118-121231/ (stored 0%)\n",
            "  adding: content/content/output/run20221118-121231/log.log (deflated 70%)\n",
            "  adding: content/content/output/run20221118-121404/ (stored 0%)\n",
            "  adding: content/content/output/run20221118-121404/log.log (deflated 70%)\n",
            "  adding: content/content/output/run20221118-122326/ (stored 0%)\n",
            "  adding: content/content/output/run20221118-122326/log.log (deflated 70%)\n",
            "  adding: content/content/output/run20221118-114658/ (stored 0%)\n",
            "  adding: content/content/output/run20221118-114658/log.log (deflated 70%)\n",
            "  adding: content/content/output/run20221118-113201/ (stored 0%)\n",
            "  adding: content/content/output/run20221118-113201/log.log (deflated 70%)\n",
            "  adding: content/content/output/run20221118-111007/ (stored 0%)\n",
            "  adding: content/content/output/run20221118-111007/log.log (deflated 70%)\n",
            "  adding: content/content/output/run20221118-121354/ (stored 0%)\n",
            "  adding: content/content/output/run20221118-121354/log.log (deflated 70%)\n",
            "  adding: content/content/Elonmusktweets.csv (deflated 58%)\n",
            "  adding: content/content/drive/ (stored 0%)\n",
            "  adding: content/content/drive/.shortcut-targets-by-id/ (stored 0%)\n",
            "  adding: content/content/drive/.file-revisions-by-id/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/106120012.jpg (deflated 15%)\n",
            "  adding: content/content/drive/MyDrive/106120012 (10).pdf (deflated 9%)\n",
            "  adding: content/content/drive/MyDrive/Copy of Computer Graphics Through OpenGL_ From Theory to Experiments ( PDFDrive.com ).pdf (deflated 1%)\n",
            "  adding: content/content/drive/MyDrive/Resume/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Resume/106120012__Amogh Sundararaman.pdf (deflated 10%)\n",
            "  adding: content/content/drive/MyDrive/Resume/106120012_Amogh Sundararaman.pdf (deflated 10%)\n",
            "  adding: content/content/drive/MyDrive/Resume/Amogh Sundararaman - Content Resume.pdf (deflated 34%)\n",
            "  adding: content/content/drive/MyDrive/Personal statement draft.pdf (deflated 2%)\n",
            "  adding: content/content/drive/MyDrive/106120012_Amogh Sundararaman (1).pdf (deflated 10%)\n",
            "  adding: content/content/drive/MyDrive/Copy of Database System Concepts, 6th Edition.pdf (deflated 36%)\n",
            "  adding: content/content/drive/MyDrive/diatomic molecule.pptx (deflated 0%)\n",
            "  adding: content/content/drive/MyDrive/106120012--Resume.pdf (deflated 9%)\n",
            "  adding: content/content/drive/MyDrive/Copy of A Textbook Of Engineering Mathematics by Usha Paul N. P. Bali.pdf (deflated 7%)\n",
            "  adding: content/content/drive/MyDrive/Emirates Airlines.docx (deflated 7%)\n",
            "  adding: content/content/drive/MyDrive/Untitled Diagram (3).drawio (deflated 22%)\n",
            "  adding: content/content/drive/MyDrive/Project Final/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Project Final/DATA.xlsx (deflated 30%)\n",
            "  adding: content/content/drive/MyDrive/Project Final/Project Code Final.py (deflated 80%)\n",
            "  adding: content/content/drive/MyDrive/Project Final/blank space/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Project Final/blank space/OVERVIEW________________.py (deflated 76%)\n",
            "  adding: content/content/drive/MyDrive/Project Final/blank space/dude.jpg (deflated 4%)\n",
            "  adding: content/content/drive/MyDrive/Project Final/blank space/test_1.py (deflated 81%)\n",
            "  adding: content/content/drive/MyDrive/Project Final/blank space/download.jpg (deflated 0%)\n",
            "  adding: content/content/drive/MyDrive/Project Final/blank space/Overviewalter.py (deflated 80%)\n",
            "  adding: content/content/drive/MyDrive/Project Final/blank space/BIN/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Project Final/blank space/BIN/BIOMASS TO ENERGY CONVERSION.pptx (deflated 2%)\n",
            "  adding: content/content/drive/MyDrive/Project Final/blank space/BIN/New folder/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Project Final/blank space/BIN/New folder/DSC_0502.JPG (deflated 0%)\n",
            "  adding: content/content/drive/MyDrive/Project Final/blank space/BIN/New folder/DSC_0500.JPG (deflated 0%)\n",
            "  adding: content/content/drive/MyDrive/Project Final/blank space/BIN/New folder/DSC_0501.JPG (deflated 0%)\n",
            "  adding: content/content/drive/MyDrive/Project Final/blank space/BIN/New folder/DSC_0505.JPG (deflated 0%)\n",
            "  adding: content/content/drive/MyDrive/Project Final/blank space/BIN/do not open/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Project Final/blank space/BIN/do not open/zerowasre.jpg (deflated 2%)\n",
            "  adding: content/content/drive/MyDrive/Project Final/blank space/BIN/do not open/sierra.png (deflated 4%)\n",
            "  adding: content/content/drive/MyDrive/Project Final/blank space/BIN/do not open/present.docx (deflated 10%)\n",
            "  adding: content/content/drive/MyDrive/Project Final/blank space/BIN/do not open/advanced-plasma-power.png (deflated 4%)\n",
            "  adding: content/content/drive/MyDrive/Project Final/blank space/BIN/do not open/Conversion of bio mass into energy.pptx (deflated 2%)\n",
            "  adding: content/content/drive/MyDrive/Project Final/blank space/BIN/do not open/Wheelabrator_Logo_Final_7705_3282.jpg (deflated 25%)\n",
            "  adding: content/content/drive/MyDrive/Project Final/blank space/BIN/do not open/presentation.docx (deflated 20%)\n",
            "  adding: content/content/drive/MyDrive/Project Final/blank space/BIN/do not open/enertek.jpg (deflated 7%)\n",
            "  adding: content/content/drive/MyDrive/Project Final/blank space/BIN/do not open/the correct update - Copy.pptx (deflated 12%)\n",
            "  adding: content/content/drive/MyDrive/Project Final/blank space/BIN/do not open/Definition of Bio mass.docx (deflated 4%)\n",
            "  adding: content/content/drive/MyDrive/Project Final/blank space/BIN/do not open/ggi.jpg (deflated 9%)\n",
            "  adding: content/content/drive/MyDrive/Project Final/blank space/BIN/do not open/Covanta_Energy.png (deflated 5%)\n",
            "  adding: content/content/drive/MyDrive/Project Final/blank space/BIN/do not open/WASTE TO ENERGY UNOFFICIAL.pptx (deflated 15%)\n",
            "  adding: content/content/drive/MyDrive/Project Final/blank space/BIN/do not open/present.ppt.docx (deflated 10%)\n",
            "  adding: content/content/drive/MyDrive/Project Final/blank space/BIN/do not open/soy farm.use when applicable.jpg (deflated 0%)\n",
            "  adding: content/content/drive/MyDrive/Project Final/blank space/BIN/do not open/tectronic.jpg (deflated 64%)\n",
            "  adding: content/content/drive/MyDrive/Project Final/blank space/BIN/do not open/the correct.pptx (deflated 12%)\n",
            "  adding: content/content/drive/MyDrive/Project Final/blank space/BIN/Modern_2009000205_MO435360002405.pdf (deflated 51%)\n",
            "  adding: content/content/drive/MyDrive/Project Final/blank space/FTSE.pdf (deflated 7%)\n",
            "  adding: content/content/drive/MyDrive/Project Final/blank space/back.png (deflated 0%)\n",
            "  adding: content/content/drive/MyDrive/Project Final/blank space/jjj.py (deflated 14%)\n",
            "  adding: content/content/drive/MyDrive/Project Final/blank space/Overview__.py (deflated 76%)\n",
            "  adding: content/content/drive/MyDrive/Project Final/Hardcopy.docx (deflated 10%)\n",
            "  adding: content/content/drive/MyDrive/Aaveg Content Important files/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Aaveg Content Important files/Questionnaire For Sorting into teams.docx (deflated 8%)\n",
            "  adding: content/content/drive/MyDrive/Aaveg Content Important files/Intro and Outro to Questionnaire.docx (deflated 20%)\n",
            "  adding: content/content/drive/MyDrive/Aaveg Content Important files/AavegGT3 (2).docx (deflated 12%)\n",
            "  adding: content/content/drive/MyDrive/Aaveg Content Important files/Error Page Tagline  Credit Tagline and Landing Page tagline.docx (deflated 21%)\n",
            "  adding: content/content/drive/MyDrive/Aaveg Content Important files/Team Descriptions.docx (deflated 9%)\n",
            "  adding: content/content/drive/MyDrive/Aaveg Content Important files/PROLOGUE.docx (deflated 19%)\n",
            "  adding: content/content/drive/MyDrive/Copy of Artificial Intelligence.pdf (deflated 2%)\n",
            "  adding: content/content/drive/MyDrive/WhatsApp Image 2021-12-18 at 3.58.29 PM (2).jpeg (deflated 0%)\n",
            "  adding: content/content/drive/MyDrive/106120012__Amogh Sundararaman (7).pdf (deflated 10%)\n",
            "  adding: content/content/drive/MyDrive/Air Asia (1).docx (deflated 7%)\n",
            "  adding: content/content/drive/MyDrive/Etihad Airways (1).docx (deflated 7%)\n",
            "  adding: content/content/drive/MyDrive/Screen Writing - Clan Vayu - Amogh Sundararaman - 106120012-converted (1).pdf (deflated 8%)\n",
            "  adding: content/content/drive/MyDrive/paymenthistoryredirecturl.pdf (deflated 19%)\n",
            "  adding: content/content/drive/MyDrive/Copy of Civil Engineering Formulas 2009.pdf (deflated 20%)\n",
            "  adding: content/content/drive/MyDrive/Copy of Engineering Physics ‚àí Theory and Practical by A. K. Katiyar, C. K. Pandey.pdf (deflated 26%)\n",
            "  adding: content/content/drive/MyDrive/Grade Sheets.pdf (deflated 0%)\n",
            "  adding: content/content/drive/MyDrive/ENIR11_Project_coordinator report.docx (deflated 11%)\n",
            "  adding: content/content/drive/MyDrive/Amogh Sundararaman/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Amogh Sundararaman/2022/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Amogh Sundararaman/2022/5 9/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Amogh Sundararaman/2022/8 8/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Amogh Sundararaman/2022/11 7/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Amogh Sundararaman/2022/20 6/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Amogh Sundararaman/2022/27 6/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Amogh Sundararaman/2022/22 8/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Amogh Sundararaman/2022/18 7/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Amogh Sundararaman/2022/OUTLINES/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Amogh Sundararaman/2022/23 5/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Amogh Sundararaman/2022/30 5/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Amogh Sundararaman/2022/13 6/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Amogh Sundararaman/2022/25 7/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Amogh Sundararaman/2022/4 7/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Amogh Sundararaman/2022/6 6/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Amogh Sundararaman/2022/26 9/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Amogh Sundararaman/2022/15 8/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Amogh Sundararaman/2022/29 8/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Amogh Sundararaman/2022/1 8/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Amogh Sundararaman/2022/9 19/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Amogh Sundararaman - Conspiracy Article writing - 106120012 - Clan Vayu-converted.pdf (deflated 5%)\n",
            "  adding: content/content/drive/MyDrive/Copy of Fulbright Study Objectives.docx (deflated 13%)\n",
            "  adding: content/content/drive/MyDrive/106120012 - NSO WRITE UP - Amogh Sundararaman-converted.pdf (deflated 9%)\n",
            "  adding: content/content/drive/MyDrive/106120012_Amogh Sundararaman_ CSIR_1,2,3,4.pdf (deflated 12%)\n",
            "  adding: content/content/drive/MyDrive/Etihad Airways.docx (deflated 7%)\n",
            "  adding: content/content/drive/MyDrive/106120012_Amogh Sundararaman (8).pdf (deflated 9%)\n",
            "  adding: content/content/drive/MyDrive/106120012 (11) (1) (3).pdf (deflated 9%)\n",
            "  adding: content/content/drive/MyDrive/SOP - Amogh Sundararaman .pdf (deflated 3%)\n",
            "  adding: content/content/drive/MyDrive/106120012 Resume.pdf (deflated 9%)\n",
            "  adding: content/content/drive/MyDrive/Qatar Airways.docx (deflated 7%)\n",
            "  adding: content/content/drive/MyDrive/106120012__Amogh Sundararaman (5).pdf (deflated 10%)\n",
            "  adding: content/content/drive/MyDrive/Personal statement draft (1).pdf (deflated 2%)\n",
            "  adding: content/content/drive/MyDrive/Copy of Computer Networking and Internet Protocol -A comprehensive Introduction.pdf (deflated 16%)\n",
            "  adding: content/content/drive/MyDrive/Photos for resume/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Photos for resume/Passport Photo Amogh.jpg (deflated 7%)\n",
            "  adding: content/content/drive/MyDrive/IM Personal Statement.docx (deflated 13%)\n",
            "  adding: content/content/drive/MyDrive/Solution.docx (deflated 8%)\n",
            "  adding: content/content/drive/MyDrive/kaviya maths assignment 2 26918 .pdf (deflated 3%)\n",
            "  adding: content/content/drive/MyDrive/Copy of Advanced Engineering Mathematics, 7th Edition by Peter V. ONeil.pdf (deflated 56%)\n",
            "  adding: content/content/drive/MyDrive/Copy of Advanced Engineering Mathematics by E Kreyszig, H Kreyszig E  J Norminton.pdf (deflated 14%)\n",
            "  adding: content/content/drive/MyDrive/matrices and determinants.pdf (deflated 0%)\n",
            "  adding: content/content/drive/MyDrive/Copy of Advanced engineering mathematics by Potter, Merle C. Aboufadel, Edward Goldberg, Jack Leonard.pdf (deflated 26%)\n",
            "  adding: content/content/drive/MyDrive/Copy of Artificial Intelligence and Soft Computing.pdf (deflated 11%)\n",
            "  adding: content/content/drive/MyDrive/106120012__Amogh Sundararaman.pdf (deflated 10%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/MEIR12 Engineering Graphics CSE CSE B/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Templates - DO NOT EDIT/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Templates - DO NOT EDIT/[Template] Matrices ( Question paper).pdf (deflated 7%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Matrices Worksheet/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Matrices Worksheet/veerapandi - Matrices ( Question paper).pdf (deflated 7%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Matrices Worksheet/20210207_232839.jpg (deflated 3%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/ATOMIC STRUCTURE / (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Straight lines Assignment/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Straight lines Assignment/Straight lines Assignment (Feb 8, 2021 10:25:05 AM).jpeg (deflated 12%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Straight lines Assignment/Straight lines Assignment (Feb 8, 2021 10:25:37 AM).jpeg (deflated 12%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Straight lines Assignment/Straight lines Assignment (Feb 8, 2021 10:25:59 AM).jpeg (deflated 12%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Current and Electrostatics/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Current and Electrostatics/Current and Electrostatics (Feb 8, 2021 12:06:24 PM).jpeg (deflated 11%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Current and Electrostatics/Current and Electrostatics (Feb 8, 2021 12:07:07 PM).jpeg (deflated 10%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Current and Electrostatics/Current and Electrostatics (Feb 8, 2021 12:07:32 PM).jpeg (deflated 11%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Current and Electrostatics/Current and Electrostatics (Feb 8, 2021 12:08:50 PM).jpeg (deflated 10%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/MR 2019 QP (3).pdf (deflated 5%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/MR 2019 QP (2).pdf (deflated 5%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/MR 2019 QP (1).pdf (deflated 5%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/MR 2019 QP.pdf (deflated 5%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/MR 2020 QP (3).pdf (deflated 20%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/MR 2020 QP (2).pdf (deflated 20%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/MR 2020 QP (1).pdf (deflated 20%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/MR 2020 QP.pdf (deflated 20%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Stats Archives.pdf (deflated 2%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/CHEMICAL BONDING.pdf (deflated 1%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/NTA TEST NO : 6 Physics Answer Script/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/NTA TEST NO : 6 Physics Answer Script/NTA TEST NO : 6 Physics Answer Script (Feb 9, 2021 8:31:26 PM).jpeg (deflated 13%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/NTA TEST NO : 6 Chemistry Answer Script/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/NTA TEST NO : 6 Chemistry Answer Script/NTA TEST NO : 6 Chemistry Answer Script (Feb 9, 2021 8:33:02 PM).jpeg (deflated 11%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/NTA TEST NO : 6 Maths Answer Script/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/NTA TEST NO : 6 Maths Answer Script/NTA TEST NO : 6 Maths Answer Script (Feb 9, 2021 8:33:37 PM).jpeg (deflated 10%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/NTA TEST NO : 6 Maths Answer Script/NTA TEST NO : 6 Maths Answer Script (Feb 9, 2021 8:34:53 PM).jpeg (deflated 11%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/NTA TEST NO : 6 Maths Answer Script/NTA TEST NO : 6 Maths Answer Script (Feb 9, 2021 8:35:31 PM).jpeg (deflated 12%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/MR and Stat / (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/MR and Stat /MR and Stat  (Feb 14, 2021 6:04:40 PM).jpeg (deflated 14%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/MR and Stat /MR and Stat  (Feb 14, 2021 6:05:01 PM).jpeg (deflated 12%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/MR and Stat /MR and Stat  (Feb 14, 2021 6:05:20 PM).jpeg (deflated 12%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/MR and Stat /MR and Stat  (Feb 14, 2021 6:06:19 PM).jpeg (deflated 12%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/SEMICONDUCTORS.pdf (deflated 9%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/sequences and series assignment/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/NTA Test No 7 Chemistry Answer Script/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/NTA Test No 7 Maths Answer Sheet/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/NTA test No 7 Physics answer script/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Gaseous state ws.pdf (deflated 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/REDOX REACTIONS-2 WS.pdf (deflated 4%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Circles-1-7 (1).pdf (deflated 10%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Circles-1-7.pdf (deflated 10%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Sequence&Series_QUESTIONS (U) (2).pdf (deflated 13%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Sequence&Series_QUESTIONS (U) (1).pdf (deflated 13%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Sequence&Series_QUESTIONS (U).pdf (deflated 13%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/ùô≤ùöëùöéùöñùöíùöåùöäùöï ùöãùöòùöóùöçùöíùöóùöê ùöôùöäùöõùöù-2.pdf (deflated 5%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/LCD 2019 QP (1).pdf (deflated 7%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/LCD 2019 QP.pdf (deflated 7%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/LCD 2020 QP.pdf (deflated 21%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Vectors Worksheet-1-7.pdf (deflated 2%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/MATRICES AND DETERMINANTS.docx.pdf (deflated 13%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Main Paper 1.pdf (deflated 26%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/JEE-Main-2018-Question-Paper-08_04_2018_SetA.pdf (deflated 26%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Mains Revision Test 1/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Mains Revision Test 1/Mains Revision Test 1 (Feb 17, 2021 12:11:56 PM).jpeg (deflated 11%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Mains Revision Test 1/Mains Revision Test 1 (Feb 17, 2021 12:12:43 PM).jpeg (deflated 10%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Mains Revision Test 1/Mains Revision Test 1 (Feb 17, 2021 12:13:52 PM).jpeg (deflated 11%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Mains Revision Test 1/Mains Revision Test 1 (Feb 17, 2021 12:15:11 PM).jpeg (deflated 11%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Mains Revision Test 1/Mains Revision Test 1 (Feb 17, 2021 12:16:22 PM).jpeg (deflated 11%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Mains Revision Test 1/Mains Revision Test 1 (Feb 17, 2021 12:40:45 PM).jpeg (deflated 10%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Mains Revision Test 1/IMG-20210217-WA0020.jpg (deflated 3%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Mains Revision Test 1/IMG-20210217-WA0005 (1).jpg (deflated 4%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Mains Revision Test 1/paper 01.pdf (deflated 3%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Mains Revision Test 1/IMG-20210217-WA0005.jpg (deflated 4%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/jee-main-2016-online-CBT-paper (1).pdf (deflated 2%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Maths - 17.02.2021/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Test -2 / (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Test -2 /IMG-20210218-WA0008.jpg (deflated 4%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Physics Solutions Test 2/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Physics Solutions Test 2/Physics Solutions Test 2 (Feb 18, 2021 12:06:20 PM).jpeg (deflated 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Physics Solutions Test 2/Physics Solutions Test 2 (Feb 18, 2021 12:07:32 PM).jpeg (deflated 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Physics Solutions Test 2/IMG-20210218-WA0015.jpg (deflated 1%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Physics Solutions Test 2/IMG-20210218-WA0009.jpg (deflated 1%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Physics Solutions Test 2/IMG-20210218-WA0011.jpg (deflated 1%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Physics Solutions Test 2/IMG-20210218-WA0017.jpg (deflated 2%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Chemistry Solutions Test - 2/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Chemistry Solutions Test - 2/Chemistry Solutions Test - 2 (Feb 18, 2021 12:09:13 PM).jpeg (deflated 1%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Chemistry Solutions Test - 2/Chemistry Solutions Test - 2 (Feb 18, 2021 12:10:01 PM).jpeg (deflated 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Chemistry Solutions Test - 2/Chemistry Solutions Test - 2 (Feb 18, 2021 12:10:33 PM).jpeg (deflated 1%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Chemistry Solutions Test - 2/IMG-20210218-WA0016 (1).jpg (deflated 3%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Chemistry Solutions Test - 2/IMG-20210218-WA0016.jpg (deflated 3%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Maths Solution in Test 2/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Maths Solution in Test 2/Maths Solution in Test 2 (Feb 18, 2021 12:11:52 PM).jpeg (deflated 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Maths Solution in Test 2/Maths Solution in Test 2 (Feb 18, 2021 12:13:04 PM).jpeg (deflated 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Maths Solution in Test 2/IMG-20210218-WA0014.jpg (deflated 3%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Maths Solution in Test 2/Maths Solution in Test 2 (Feb 18, 2021 12:14:03 PM).jpeg (deflated 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Maths Solution in Test 2/IMG-20210218-WA0013.jpg (deflated 2%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Maths Solution in Test 2/IMG-20210218-WA0010.jpg (deflated 4%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/NCERT-Class-12-Chemistry-Part-1.pdf (deflated 15%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Test - 3 / (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Test - 3 /Test - 3  (Feb 19, 2021 11:59:55 AM).jpeg (deflated 11%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Test - 3 /Test - 3  (Feb 19, 2021 12:00:36 PM).jpeg (deflated 11%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Test - 3 /Test - 3  (Feb 19, 2021 12:01:04 PM).jpeg (deflated 11%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Test - 3 /IMG-20210219-WA0000.jpg (deflated 2%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Test - 3 /Test - 3  (Feb 19, 2021 12:01:47 PM).jpeg (deflated 11%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Test - 3 /Test - 3  (Feb 19, 2021 12:02:39 PM).jpeg (deflated 11%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Test - 3 /Test - 3  (Feb 19, 2021 12:03:08 PM).jpeg (deflated 11%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Test - 3 /Test - 3  (Feb 19, 2021 12:03:34 PM).jpeg (deflated 11%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Test - 3 /Test - 3  (Feb 19, 2021 12:04:00 PM).jpeg (deflated 10%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Test - 3 /Test - 3  (19 Feb 2021 12:34:31 pm).jpeg (deflated 3%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/chemistry_jee_main_2019_gaseous_state.pdf (deflated 8%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/gaseous_state_questions_jee_main_2020_chapterwise_question.pdf (deflated 27%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/2019-20 atomic structure.pdf (deflated 19%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/jee-main-2016-paper-Code-H.pdf (deflated 32%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Test - 4/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Test - 4/phy.pdf (deflated 15%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Test - 4/maths.pdf (deflated 11%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Test - 4/che.pdf (deflated 12%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Test - 4/IMG-20210220-WA0002.jpg (deflated 2%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Chemistry solutions test 4/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Chemistry solutions test 4/chemistry.pdf (deflated 4%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Chemistry solutions test 4/che.pdf (deflated 12%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Physics Solutions Test 4/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Physics Solutions Test 4/IMG-20210220-WA0004.jpg (deflated 3%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Physics Solutions Test 4/IMG-20210220-WA0003.jpg (deflated 2%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Physics Solutions Test 4/phy.pdf (deflated 15%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Maths Solutions Test / (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Maths Solutions Test /maths.pdf (deflated 11%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Redox.pdf (deflated 5%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/periodic properties_jee mains 2019,2020_worksheet.pdf (deflated 2%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/WhatsApp Image 2021-02-20 at 19.44.22.jpeg (deflated 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/WhatsApp Image 2021-02-20 at 19.44.36.jpeg (deflated 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Physics - 21.02.2021/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Physics - 21.02.2021/IMG-20210221-WA0015.jpg (deflated 9%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Physics - 21.02.2021/IMG-20210221-WA0014.jpg (deflated 9%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Physics - 21.02.2021/Physics - 21.02.2021 (Feb 21, 2021 1:15:32 PM).jpeg (deflated 10%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Physics - 21.02.2021/Physics - 21.02.2021 (Feb 21, 2021 1:16:21 PM).jpeg (deflated 10%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Physics - 21.02.2021/IMG-20210221-WA0075.jpg (deflated 2%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Jee-main-online-paper-1-2015.pdf (deflated 1%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Exam (22nd Feb)/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Exam (22nd Feb)/Exam (22nd Feb) (Feb 23, 2021 11:55:07 AM).jpeg (deflated 10%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Exam (22nd Feb)/Exam (22nd Feb) (Feb 23, 2021 11:55:38 AM).jpeg (deflated 11%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Exam (22nd Feb)/Exam (22nd Feb) (Feb 23, 2021 11:56:07 AM).jpeg (deflated 11%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Exam (22nd Feb)/Exam (22nd Feb) (Feb 23, 2021 11:56:58 AM).jpeg (deflated 11%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Exam (22nd Feb)/Exam (22nd Feb) (Feb 23, 2021 11:57:23 AM).jpeg (deflated 11%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Exam (22nd Feb)/Exam (22nd Feb) (Feb 23, 2021 11:58:39 AM).jpeg (deflated 11%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Exam (22nd Feb)/IMG-20210223-WA0012.jpg (deflated 6%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/2019-20 atomic structure (2).pdf (deflated 19%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/periodic properties_jee mains 2019,2020_worksheet (1).pdf (deflated 2%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/gaseous_state_questions_jee_main_2020_chapterwise_question (1).pdf (deflated 27%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/kinetics 1920 q paper.pdf (deflated 20%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/solutions+coll Prop.pdf (deflated 5%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Chem Equilibr Final (2) (1).pdf (deflated 6%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/ALLEN DLP JEE MAINS LEADER COURSE  MAJOR TEST 8 PAPER  WITH SOLUTION TEST DATE 15-03-2020-jeemain.guru.pdf (deflated 5%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/WhatsApp Image 2021-03-20 at 20.27.57.jpeg (deflated 1%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/WhatsApp Image 2021-03-21 at 11.56.23.jpeg (deflated 1%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Electrochemistry assignment/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Surface Chemistry - Worksheet 1 / (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Blank Quiz.gform\n",
            "zip warning: Operation not supported\n",
            "\tzip warning: could not open for reading: content/content/drive/MyDrive/Classroom/IGNITTE Group 1/Blank Quiz.gform\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/class 1 lcd.pdf (deflated 29%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/Untitled drawing.gdraw\n",
            "zip warning: Operation not supported\n",
            "\tzip warning: could not open for reading: content/content/drive/MyDrive/Classroom/Impulse Group 2/Untitled drawing.gdraw\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/LCD.pdf (deflated 6%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/worksheet gaseous satate.pdf (deflated 6%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/MOVING CHARGES AND MAGNETISM.pdf (deflated 5%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/CN QNS.docx (deflated 5%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/Complex number doubts.pdf (deflated 8%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/Matrices ( Question paper).pdf (deflated 7%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/LCD Ans Key.pdf (deflated 4%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/Determinants (Question paper).pdf (deflated 8%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/MAGNETISM AND EMI.pptx (deflated 1%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/WORKSHEET-MOVING CHARGES AND MAGNETISM.pdf (deflated 5%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/WORKSHEET -MOVING CHARGES AND MAGNETISM AND MAGNETISM AND MATTER/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/WORKSHEET -MOVING CHARGES AND MAGNETISM AND MAGNETISM AND MATTER/doc 1 (1).pdf (deflated 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/WORKSHEET -MOVING CHARGES AND MAGNETISM AND MAGNETISM AND MATTER/Swetha Shanmugam - WORKSHEET -MOVING CHARGES AND MAGNETISM AND MAGNETISM AND MATTER (2).gdoc\n",
            "zip warning: Operation not supported\n",
            "\tzip warning: could not open for reading: content/content/drive/MyDrive/Classroom/Impulse Group 2/WORKSHEET -MOVING CHARGES AND MAGNETISM AND MAGNETISM AND MATTER/Swetha Shanmugam - WORKSHEET -MOVING CHARGES AND MAGNETISM AND MAGNETISM AND MATTER (2).gdoc\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/WORKSHEET -MOVING CHARGES AND MAGNETISM AND MAGNETISM AND MATTER/Swetha Shanmugam - WORKSHEET -MOVING CHARGES AND MAGNETISM AND MAGNETISM AND MATTER (1).gdoc\n",
            "zip warning: Operation not supported\n",
            "\tzip warning: could not open for reading: content/content/drive/MyDrive/Classroom/Impulse Group 2/WORKSHEET -MOVING CHARGES AND MAGNETISM AND MAGNETISM AND MATTER/Swetha Shanmugam - WORKSHEET -MOVING CHARGES AND MAGNETISM AND MAGNETISM AND MATTER (1).gdoc\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/WORKSHEET -MOVING CHARGES AND MAGNETISM AND MAGNETISM AND MATTER/Swetha Shanmugam - WORKSHEET -MOVING CHARGES AND MAGNETISM AND MAGNETISM AND MATTER.gdoc\n",
            "zip warning: Operation not supported\n",
            "\tzip warning: could not open for reading: content/content/drive/MyDrive/Classroom/Impulse Group 2/WORKSHEET -MOVING CHARGES AND MAGNETISM AND MAGNETISM AND MATTER/Swetha Shanmugam - WORKSHEET -MOVING CHARGES AND MAGNETISM AND MAGNETISM AND MATTER.gdoc\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/WORKSHEET -MOVING CHARGES AND MAGNETISM AND MAGNETISM AND MATTER/doc 1.pdf (deflated 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/Maths answer sheet- NTA Test No 5/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/Maths answer sheet- NTA Test No 5/Maths answer sheet- NTA Test No 5 (04-Feb-2021 10:12:52 pm).jpeg (deflated 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/Maths answer sheet- NTA Test No 5/Maths answer sheet- NTA Test No 5 (04-Feb-2021 10:13:49 pm).jpeg (deflated 1%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/Maths answer sheet- NTA Test No 5/Maths answer sheet- NTA Test No 5 (04-Feb-2021 10:14:35 pm).jpeg (deflated 1%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/Maths answer sheet- NTA Test No 5/maths.pdf (deflated 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/Physics Answer Sheet - NTA Test No 5/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/Chemistry Answer Sheet NTA Test No 5/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/SOLUTIONS WORKSHEET-MOVING CHARGES AND MAGNETISM.pdf (deflated 7%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/NTA TEST NO 5 SOLUTIONS_compressed.pdf (deflated 3%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/Physics Practice questions - NTA test 5 / (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/Physics Practice questions - NTA test 5 /IMG_20210206_052158.jpg (deflated 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/CHEMICAL KINETICS worksheet.pdf (deflated 8%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/edg.png (deflated 3%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/sum.png (deflated 5%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/DOC-20210206-WA0019..pdf (deflated 13%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/Worksheet on capacitor+current electricity+magnetism.docx (deflated 4%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/Current Electricity/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/Current Electricity/Current Electricity (07-Feb-2021 5:01:40 pm).jpeg (deflated 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/Current Electricity/current electricity.pdf (deflated 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/Problems- Seq and Series/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/Problems- Seq and Series/Problems- Seq amd Series (07-Feb-2021 6:01:28 pm).jpeg (deflated 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/Problems- Seq and Series/Problems- Seq amd Series (07-Feb-2021 6:02:34 pm).jpeg (deflated 1%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/Problems- Seq and Series/Problems- Seq amd Series (07-Feb-2021 6:03:35 pm).jpeg (deflated 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/Problems- Seq and Series/seq and series.pdf (deflated 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/JEE SL 2019-20.pdf (deflated 16%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/St Lines REVISION.pdf (deflated 33%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/JEE SL Solutions 2019-20.pdf (deflated 33%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/answers_worksheet_shivam.pdf (deflated 4%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/Stats Worksheet (1).pdf (deflated 3%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/Stats Worksheet.pdf (deflated 3%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/MR 2019 QP (1).pdf (deflated 5%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/MR 2019 QP.pdf (deflated 5%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/MR 2020 QP (1).pdf (deflated 20%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/MR 2020 QP.pdf (deflated 20%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/Stats Archives.pdf (deflated 2%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/qe revision.pdf (deflated 27%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/QUADRATIC EQUATIONS (QUESTION PAPER).pdf (deflated 1%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/current_qno10_figure.pdf (deflated 7%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/NTA TEST NO : 6 Physics Answer Script/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/NTA TEST NO : 6 Physics Answer Script/NTA TEST NO : 6 Physics Answer Script (09-Feb-2021 7:15:13 pm).jpeg (deflated 1%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/NTA TEST NO : 6 Physics Answer Script/physics.pdf (deflated 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/NTA TEST NO : 6 Chemistry Answer Script/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/NTA TEST NO : 6 Chemistry Answer Script/NTA TEST NO : 6 Chemistry Answer Script (09-Feb-2021 7:16:13 pm).jpeg (deflated 1%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/NTA TEST NO : 6 Chemistry Answer Script/chemistry .pdf (deflated 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/NTA TEST NO : 6 Maths Answer Script/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/NTA TEST NO : 6 Maths Answer Script/NTA TEST NO : 6 Maths Answer Script (09-Feb-2021 7:17:52 pm).jpeg (deflated 1%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/NTA TEST NO : 6 Maths Answer Script/maths  (1).pdf (deflated 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/NTA TEST NO : 6 Maths Answer Script/maths .pdf (deflated 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/NTA Test No 6 Solutions.pdf (deflated 8%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/MR 2019 Answer Key.pdf (deflated 10%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/MR 2020 Answer Key.pdf (deflated 34%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/Practice Problems - MR and Stat/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/Practice Problems - MR and Stat/statistics_ 3rd  sum.pdf (deflated 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/SEMICONDUCTORS.pptx (deflated 2%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/SEMICONDUCTORS.pdf (deflated 9%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/NTA TEST NO 7 Maths Answer script/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/NTA TEST NO 7 Maths Answer script/Adobe Scan 12-Feb-2021.pdf (deflated 1%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/NTA TEST NO 7 Maths Answer script/maths (1).pdf (deflated 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/NTA TEST NO 7 Maths Answer script/maths.pdf (deflated 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/NTA Test No 7 Physics Answer Script/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/NTA Test No 7 Physics Answer Script/Adobe Scan 12-Feb-2021 (1).pdf (deflated 1%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/NTA test No 7 Chemistry Answer script/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/NTA test No 7 Chemistry Answer script/Adobe Scan 12-Feb-2021 (2).pdf (deflated 1%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/NTA test No 7 Chemistry Answer script/chemistry .pdf (deflated 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/Circles (1).pdf (deflated 9%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/Circles.pdf (deflated 9%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/Sequence&Series_QUESTIONS (U).pdf (deflated 13%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/Circles Revision.pdf (deflated 28%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/Atomic Structure/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/Atomic Structure/Atomic stricture .pdf (deflated 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/DOPPLER EFFECT,AC,MOI.pdf (deflated 4%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/Redox Reactions.pdf (deflated 6%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/Practice Problems - Circles/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/Practice Problems - Circles/Practice Problems - Circles (14-Feb-2021 8:03:11 pm).jpeg (deflated 1%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/Practice Problems - Circles/Practice Problems - Circles (14-Feb-2021 8:03:30 pm).jpeg (deflated 1%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/Practice Problems - Circles/Practice Problems - Circles (14-Feb-2021 8:03:55 pm).jpeg (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/Practice Problems - Circles/circles (1).pdf (deflated 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/Practice Problems - Circles/circles.pdf (deflated 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/Circles Solutions 3 to 9.pdf (deflated 12%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/REDOX worksheet / (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/REDOX worksheet /Redox reaction.pdf (deflated 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/REDOX worksheet /Adobe Scan 16-Feb-2021.pdf (deflated 1%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/Solutions Circles qns 3 to 9/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/Solutions Circles qns 3 to 9/circles.pdf (deflated 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/LCD 2019 QP (1).pdf (deflated 7%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/LCD 2019 QP.pdf (deflated 7%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/LCD 2020 QP (1).pdf (deflated 21%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/LCD 2020 QP.pdf (deflated 21%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/LCD 2020 Answer Key (1).pdf (deflated 30%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/LCD 2020 Answer Key.pdf (deflated 30%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/LCD 2019 Answer Key (1).pdf (deflated 10%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/LCD 2019 Answer Key.pdf (deflated 10%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/U&D worksheet.pdf (deflated 1%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/U&D solns.pdf (deflated 6%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/AC worksheet.pdf (deflated 3%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/AC solns.pdf (deflated 2%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/MOI solns.pdf (deflated 3%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/MOI worksheet.pdf (deflated 1%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/Practice Problems - LCD/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/Practice Problems - LCD/limits.pdf (deflated 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/MATRICES AND DETERMINANTS (2).docx.pdf (deflated 13%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/MATRICES AND DETERMINANTS (1).docx.pdf (deflated 13%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/MATRICES AND DETERMINANTS.docx.pdf (deflated 13%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/Practice Problems -  Mat Det/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/Practice Problems -  Mat Det/Practice Problems -  MatDet (19-Feb-2021 10:55:39 pm).jpeg (deflated 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/JEE-Main-2018-Question-Paper-08_04_2018_SetA.pdf (deflated 26%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/Today test question paper/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/Today test question paper/test 1.pdf (deflated 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/jee-main-2016-online-CBT-paper (1).pdf (deflated 2%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/jee-main-2017-online-CBT-paper-Dt-09-04-2017.pdf (deflated 14%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/Practice Problems - QE/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/Practice Problems - QE/Adobe Scan 18-Feb-2021.pdf (deflated 1%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/Blank Quiz (1).gform\n",
            "zip warning: Operation not supported\n",
            "\tzip warning: could not open for reading: content/content/drive/MyDrive/Classroom/Impulse Group 2/Blank Quiz (1).gform\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/Test -2 / (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/Test -2 /test no.2.pdf (deflated 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/Test -2 /Adobe Scan 19-Feb-2021.pdf (deflated 1%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/Physics SOlutions- test 2/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/Physics SOlutions- test 2/Physics SOlutions- test 2 (18-Feb-2021 12:08:26 pm).jpeg (deflated 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/Chemistry Solutions test 2/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/Chemistry Solutions test 2/Chemistry Solutions test 2 (18-Feb-2021 12:09:29 pm).jpeg (deflated 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/Chemistry Solutions test 2/chem.pdf (deflated 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/Maths solutions test 2/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/Current, AC, Units and Dimensions.pdf (deflated 21%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/Test solutions.pdf (deflated 3%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/Seq and Series , Stat/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/Test - 3/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/Test - 3/test .no.3.pdf (deflated 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/Test - 3/Test - 3 (19-Feb-2021 12:00:37 pm).jpeg (deflated 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/Test - 3/Test - 3 (19-Feb-2021 12:02:14 pm).jpeg (deflated 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/jee-main-2016-paper-Code-H.pdf (deflated 32%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/Isomerism.gform\n",
            "zip warning: Operation not supported\n",
            "\tzip warning: could not open for reading: content/content/drive/MyDrive/Classroom/Impulse Group 2/Isomerism.gform\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/mat det , MR/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/mat det , MR/home work.pdf (deflated 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/Test - 4/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/Test - 4/test.no.4.pdf (deflated 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/Test - 4/Test - 4 (20-Feb-2021 12:01:59 pm).jpeg (deflated 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/Physics solutions test 4/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/Physics solutions test 4/phy.pdf (deflated 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/Chemistry Solutions test 4/ (stored 0%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/Chemistry Solutions test 4/Adobe Scan 20-Feb-2021 (1).pdf (deflated 1%)\n",
            "  adding: content/content/drive/MyDrive/Classroom/Impulse Group 2/Chemistry Solutions test 4/chem.pdf"
          ]
        }
      ]
    }
  ]
}