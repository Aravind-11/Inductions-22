{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1wV1KxSO433Tf7XSuUg51wkmmvpo10Lb2","timestamp":1668953045531},{"file_id":"1UiIMWgtT_MgEunpcMFSRVHPWo78sAfj_","timestamp":1668531522576}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import keras\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator,img_to_array, load_img\n","from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dropout, BatchNormalization, Dense, Flatten, Activation\n","from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras.regularizers import l2\n","from tensorflow.keras.optimizers import Adam\n","import numpy as np\n","import os"],"metadata":{"id":"QpWXTTVvdk-P","executionInfo":{"status":"ok","timestamp":1668974781372,"user_tz":-330,"elapsed":2469,"user":{"displayName":"Ganesh A R","userId":"00915869214356693532"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["len(validation)"],"metadata":{"id":"psvMYz-7lBDZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["datagen = ImageDataGenerator(\n","        rotation_range=40,\n","        width_shift_range=0.2,\n","        height_shift_range=0.2,\n","        shear_range=0.2,\n","        zoom_range=0.2,\n","        horizontal_flip=True,\n","        fill_mode='nearest') \n","\n","############################\n","#Single image augmentation \n","img = load_img('/content/drive/MyDrive/Projects /Image Classifier Avengers/thor/Thor.jpg')  \n","img2 = load_img('/content/drive/MyDrive/Projects /Image Classifier Avengers/thor/thor.jpg')  \n","# uses Pillow in the backend, so need to convert to array\n","\n","x = img_to_array(img)  # this is a Numpy array with shape (3, 150, 150)\n","x = x.reshape((1,) + x.shape)  # this is a Numpy array with shape (1, 3, 150, 150)\n","y = img_to_array(img2)  # this is a Numpy array with shape (3, 150, 150)\n","y = y.reshape((1,) + y.shape)  # this is a Numpy array with shape (1, 3, 150, 150)\n","\n","# the .flow() command below generates batches of randomly transformed images\n","# and saves the results to the `preview/` directory\n","i = 0\n","j=0\n","for batch in datagen.flow(x, batch_size=1,  \n","                          save_to_dir='/content/drive/MyDrive/Projects /Image Classifier Avengers/thor_data/train', save_prefix='Thor', save_format='jpg'):\n","    i += 1\n","    if i > 200:\n","        break  # otherwise the generator would loop indefinitely\n","for batch in datagen.flow(y, batch_size=1,  \n","                          save_to_dir='/content/drive/MyDrive/Projects /Image Classifier Avengers/thor_data/validation', save_prefix='thor', save_format='jpg'):\n","    j += 1\n","    if j > 72:\n","        break  # otherwise the generator would loop indefinitely\n","\n","##########################"],"metadata":{"id":"-naNE00OFKeu","executionInfo":{"status":"ok","timestamp":1668974852634,"user_tz":-330,"elapsed":59316,"user":{"displayName":"Ganesh A R","userId":"00915869214356693532"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["from keras.preprocessing.image import ImageDataGenerator\n","from keras.models import Sequential\n","from keras.layers import Conv2D, MaxPooling2D\n","from keras.layers import Activation, Dropout, Flatten, Dense\n","#from keras import backend as K\n","\n","\n","SIZE = 150\n","###2 conv and pool layers. with some normalization and drops in between.\n","\n","INPUT_SHAPE = (SIZE, SIZE, 3)   #change to (SIZE, SIZE, 3)\n","\n","model = Sequential()\n","model.add(Conv2D(32, (3, 3), input_shape=INPUT_SHAPE))\n","model.add(Activation('relu'))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","model.add(Conv2D(32, (3, 3)))\n","model.add(Activation('relu'))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","model.add(Conv2D(64, (3, 3)))\n","model.add(Activation('relu'))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","model.add(Flatten())\n","model.add(Dense(64))\n","model.add(Activation('relu'))\n","model.add(Dropout(0.5))\n","model.add(Dense(1))\n","model.add(Activation('softmax'))\n","\n","model.compile(loss='categorical_crossentropy',\n","              optimizer='adam',\n","              metrics=['accuracy'])\n","\n","print(model.summary())    "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CvAiXMHUHvz4","executionInfo":{"status":"ok","timestamp":1668974874990,"user_tz":-330,"elapsed":834,"user":{"displayName":"Ganesh A R","userId":"00915869214356693532"}},"outputId":"2271c16b-bf3b-4f09-f27b-d27aadaf6621"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_4\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_12 (Conv2D)          (None, 148, 148, 32)      896       \n","                                                                 \n"," activation_20 (Activation)  (None, 148, 148, 32)      0         \n","                                                                 \n"," max_pooling2d_12 (MaxPoolin  (None, 74, 74, 32)       0         \n"," g2D)                                                            \n","                                                                 \n"," conv2d_13 (Conv2D)          (None, 72, 72, 32)        9248      \n","                                                                 \n"," activation_21 (Activation)  (None, 72, 72, 32)        0         \n","                                                                 \n"," max_pooling2d_13 (MaxPoolin  (None, 36, 36, 32)       0         \n"," g2D)                                                            \n","                                                                 \n"," conv2d_14 (Conv2D)          (None, 34, 34, 64)        18496     \n","                                                                 \n"," activation_22 (Activation)  (None, 34, 34, 64)        0         \n","                                                                 \n"," max_pooling2d_14 (MaxPoolin  (None, 17, 17, 64)       0         \n"," g2D)                                                            \n","                                                                 \n"," flatten_4 (Flatten)         (None, 18496)             0         \n","                                                                 \n"," dense_8 (Dense)             (None, 64)                1183808   \n","                                                                 \n"," activation_23 (Activation)  (None, 64)                0         \n","                                                                 \n"," dropout_4 (Dropout)         (None, 64)                0         \n","                                                                 \n"," dense_9 (Dense)             (None, 1)                 65        \n","                                                                 \n"," activation_24 (Activation)  (None, 1)                 0         \n","                                                                 \n","=================================================================\n","Total params: 1,212,513\n","Trainable params: 1,212,513\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n"]}]},{"cell_type":"code","source":["batch_size = 32\n","#Let's prepare our data. We will use .flow_from_directory() \n","#to generate batches of image data (and their labels) \n","#directly from our png in their respective folders.\n","\n","# this is the augmentation configuration we will use for training\n","train_datagen = ImageDataGenerator(\n","        rescale=1./255,\n","        rotation_range=45,\n","        shear_range=0.2,\n","        zoom_range=0.2,\n","        horizontal_flip=True)\n","\n","# this is the augmentation configuration we will use for validation:\n","# only rescaling. But you can try other operations\n","validation_datagen = ImageDataGenerator(rescale=1./255)\n","train_directory = '/content/drive/MyDrive/Projects /Image Classifier Avengers/thor_data/train'\n","validation_directory = '/content/drive/MyDrive/Projects /Image Classifier Avengers/thor_data/validation'\n","# this is a generator that will read pictures found in\n","# subfolers of 'data/train', and indefinitely generate\n","# batches of augmented image data\n","\n","train_generator = train_datagen.flow_from_directory(train_directory,\n","                                                    target_size=(150,150),\n","                                                    batch_size=batch_size,\n","                                                    class_mode='categorical',\n","                                                    shuffle=True)\n","\n","validation_generator = validation_datagen.flow_from_directory(validation_directory,\n","                                                              target_size=(150,150),\n","                                                              batch_size=batch_size,\n","                                                              class_mode='categorical',\n","                                                              shuffle=True)\n","\n","\n","#Add checkpoints \n","from keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger\n","#filepath='saved_models/models.h5'\n","\n","filepath=\"/content/drive/MyDrive/Projects /Image Classifier Avengers/saved_models/weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\" #File name includes epoch and validation accuracy.\n","checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n","\n","early_stop = EarlyStopping(monitor ='var_loss', patience =3, verbose =1)\n","\n","log_csv = CSVLogger('my_logs.csv', separator =',',append = False)\n","\n","callbacks_list = [checkpoint, log_csv, early_stop]\n","\n","#We can now use these generators to train our model. \n","model.fit_generator(\n","        train_generator,\n","        steps_per_epoch=3000 // batch_size,    #The 2 slashes division return rounded integer\n","        epochs=5,\n","        callbacks = callbacks_list,\n","        validation_data = validation_generator,\n","        validation_steps = 2000 // batch_size)\n","model.save('/content/drive/MyDrive/Projects /Image Classifier Avengers/saved_models/thor.h5')  # always save your weights after training or during training\n","#####################################################"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":483},"id":"ft1MR4OjH1Hu","executionInfo":{"status":"error","timestamp":1668975659539,"user_tz":-330,"elapsed":929,"user":{"displayName":"Ganesh A R","userId":"00915869214356693532"}},"outputId":"008b0acc-4109-4dcc-fa31-4ea698a196d4"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 0 images belonging to 0 classes.\n","Found 0 images belonging to 0 classes.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:56: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-29-6266f8fbebee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallbacks_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         validation_steps = 2000 // batch_size)\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Projects /Image Classifier Avengers/saved_models/thor.h5'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# always save your weights after training or during training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;31m#####################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2272\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2273\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2274\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   2275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2276\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mdoc_controls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_not_generate_docs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/preprocessing/image.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    100\u001b[0m       raise ValueError('Asked to retrieve element {idx}, '\n\u001b[1;32m    101\u001b[0m                        \u001b[0;34m'but the Sequence '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                        'has length {length}'.format(idx=idx, length=len(self)))\n\u001b[0m\u001b[1;32m    103\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m       \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_batches_seen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Asked to retrieve element 0, but the Sequence has length 0"]}]},{"cell_type":"code","source":["validation_datagen"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hj--I0Wxlb3J","executionInfo":{"status":"ok","timestamp":1668975100805,"user_tz":-330,"elapsed":820,"user":{"displayName":"Ganesh A R","userId":"00915869214356693532"}},"outputId":"b1ec66a1-7887-48d4-ebd2-b65c183127de"},"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<keras.preprocessing.image.ImageDataGenerator at 0x7f33a3167c50>"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":["\n","#Add checkpoints \n","from keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger\n","#filepath='saved_models/models.h5'\n","\n","filepath=\"/content/drive/MyDrive/Projects /Image Classifier Avengers/saved_models/weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\" #File name includes epoch and validation accuracy.\n","checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n","\n","early_stop = EarlyStopping(monitor ='var_loss', patience =3, verbose =1)\n","\n","log_csv = CSVLogger('my_logs.csv', separator =',',append = False)\n","\n","callbacks_list = [checkpoint, log_csv, callbacks_list]\n","\n","#We can now use these generators to train our model. \n","model.fit_generator(\n","        train_generator,\n","        steps_per_epoch=20 // batch_size,    #The 2 slashes division return rounded integer\n","        epochs=5,\n","        callbacks = [checkpoint, log_csv, callbacks_list],\n","        validation_data = validation_generator,\n","        validation_steps = 200 // batch_size)\n","model.save('/content/drive/MyDrive/Projects /Image Classifier Avengers/saved_models/malaria_augmented_model.h5')  # always save your weights after training or during training\n","#####################################################\n","\n","\"\"\"\n","#To continue training, by modifying weights to existing model.\n","#The saved model can be reinstated.\n","from keras.models import load_model\n","new_model = load_model('malaria_augmented_model.h5')\n","results = new_model.evaluate_generator(validation_generator)\n","print(\" validation loss and accuracy are\", results)\n","new_model.fit_generator(\n","        train_generator,\n","        steps_per_epoch=2000 // batch_size,    #The 2 slashes division return rounded integer\n","        epochs=5,\n","        validation_data=validation_generator,\n","        validation_steps=800 // batch_size,\n","        callbacks=callbacks_list)\n","model.save('malaria_augmented_model_updated.h5') \n","\"\"\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":465},"id":"o-QByPPeW6nO","executionInfo":{"status":"error","timestamp":1668973539184,"user_tz":-330,"elapsed":1656,"user":{"displayName":"Ganesh A R","userId":"00915869214356693532"}},"outputId":"855ffc70-8c41-4c0c-d2e0-ae336b477adc"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-5d8edd6045a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_csv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         validation_steps = 200 // batch_size)\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Projects /Image Classifier Avengers/saved_models/malaria_augmented_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# always save your weights after training or during training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m#####################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2272\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2273\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2274\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   2275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2276\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mdoc_controls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_not_generate_docs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/preprocessing/image.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    100\u001b[0m       raise ValueError('Asked to retrieve element {idx}, '\n\u001b[1;32m    101\u001b[0m                        \u001b[0;34m'but the Sequence '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                        'has length {length}'.format(idx=idx, length=len(self)))\n\u001b[0m\u001b[1;32m    103\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m       \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_batches_seen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Asked to retrieve element 0, but the Sequence has length 0"]}]},{"cell_type":"code","source":["len(validation_generator)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rpXKeP1ilLsB","executionInfo":{"status":"ok","timestamp":1668973648254,"user_tz":-330,"elapsed":679,"user":{"displayName":"Ganesh A R","userId":"00915869214356693532"}},"outputId":"0192069a-23e9-4fb8-f9bb-ba7c637048a4"},"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PX4SH5G_EXKB","executionInfo":{"status":"ok","timestamp":1668965080406,"user_tz":-330,"elapsed":39530,"user":{"displayName":"Ganesh A R","userId":"00915869214356693532"}},"outputId":"fc5de844-f3b9-4fed-afc2-79f11d4d03d7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["validation_datagen"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zoffwCqBkRwH","executionInfo":{"status":"ok","timestamp":1668956667073,"user_tz":-330,"elapsed":4,"user":{"displayName":"Ganesh A R","userId":"00915869214356693532"}},"outputId":"ea556c88-a000-4a43-b1d6-82c378f868c4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<keras.preprocessing.image.ImageDataGenerator at 0x7f1436667d50>"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","source":["https://colab.research.google.com/drive/1UiIMWgtT_MgEunpcMFSRVHPWo78sAfj_#scrollTo=xym-1cvfWdFe"],"metadata":{"id":"j6F16V0kayG7"},"execution_count":null,"outputs":[]}]}